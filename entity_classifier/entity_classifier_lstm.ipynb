{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "entity_classifier_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo7CtpewFt51"
      },
      "source": [
        "Description: \n",
        "First we train a Bi-LSTM character level classifier which is used to classify the entire input string into one of 5 categories. After classification, we try to group the entity with the already present entities according to simialrity criterias. If the entity is not similar enough to the already present entities it will be recorded as a new entity and assigned a new group id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3ktt5nmDCEq"
      },
      "source": [
        "!pip install jellyfish\n",
        "!pip install rapidfuzz\n",
        "!pip install autocorrect\n",
        "!pip install spacy==2.2.4\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2PakEB-nUln"
      },
      "source": [
        "import en_core_web_lg\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "\n",
        "from scipy import spatial\n",
        "from autocorrect import Speller\n",
        "\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSY7fMTgDwTp"
      },
      "source": [
        "import string\n",
        "from random import randrange\n",
        "import random  # used to create random separator\n",
        "import string  # this is used to create the vocab of all the characters , stored in the variable 'vocab'\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "import csv\n",
        "import pickle\n",
        "\n",
        "# below packages are used to create the model\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Bidirectional, TimeDistributed, Input, Masking\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n070HCcGQBZe"
      },
      "source": [
        "# max length of the input\n",
        "maxlen=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9v4nP5R5mPa"
      },
      "source": [
        "# load the csv data file\n",
        "df=pd.read_csv('/content/datafile_lstm.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1c5g1hyRg6V"
      },
      "source": [
        "# shuffle data frame\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl0w9Aj12BYB"
      },
      "source": [
        "# split the dataframe into train and test\n",
        "df1=df.iloc[100:500,:]\n",
        "df2=df.iloc[:100,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv2_qn6e13xF"
      },
      "source": [
        "# Dictionary contains the Labels\n",
        "labels_dict = {  # we are keeping the value part as strings as it easy to generate labels , we could just give len(string)*label value to generate the label\n",
        "    'company_name': 0,\n",
        "    'company_address': 1,\n",
        "    'serial_number': 2,\n",
        "    'physical_good': 3,\n",
        "    'location': 4\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBGS7WJP18UR",
        "outputId": "2b0e5d7e-398a-4efa-ec2d-7970b3856065"
      },
      "source": [
        "# create reverse dictionary of label placeholders\n",
        "reversed_labels = {value: key for (key, value) in labels_dict.items()}\n",
        "reversed_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'company_name',\n",
              " 1: 'company_address',\n",
              " 2: 'serial_number',\n",
              " 3: 'physical_good',\n",
              " 4: 'location'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2LvIHDsD50q"
      },
      "source": [
        "# define the vocab\n",
        "vocab = list(string.whitespace + string.digits +\n",
        "             string.ascii_lowercase + string.punctuation)  # total 74 characters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxqdzJTkPzZH",
        "outputId": "8a304f34-7795-4bac-c8d1-4426f038fb24"
      },
      "source": [
        "vocab_size=len(vocab)\n",
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTje8-NnEJE1"
      },
      "source": [
        "# create the Tokenizer object, word_index contaning all the chars in the vocab\n",
        "# basically this will encode all the characters into numbers, so each character defined in our vocab will be represented by a number\n",
        "# vocab is just a list of tokens\n",
        "tokenizer = Tokenizer(num_words=len(vocab), filters='',\n",
        "                      char_level=True, oov_token='<oov>')\n",
        "tokenizer.fit_on_texts(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSHrIukISxdE"
      },
      "source": [
        "#train\n",
        "train_sentences=list(df1['text'])\n",
        "train_labels=list(df1['category'])\n",
        "train_sequences=tokenizer.texts_to_sequences(train_sentences)\n",
        "train_padded=pad_sequences(train_sequences, padding='post',maxlen=maxlen, truncating='pre') #maxlen decides the maximum length of the sequence\n",
        "train_labels=[labels_dict[item] for item in train_labels ]\n",
        "train_labels = np.asarray(train_labels, dtype=np.int32)\n",
        "train_label_one_hot = tf.keras.utils.to_categorical(train_labels, dtype='int32')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRmyMQFhR6Hz"
      },
      "source": [
        "#test\n",
        "test_sentences=list(df2['text'])\n",
        "test_labels=list(df2['category'])\n",
        "test_sequences=tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded=pad_sequences(test_sequences, padding='post',maxlen=maxlen, truncating='pre') #maxlen decides the maximum length of the sequence\n",
        "test_labels=[labels_dict[item] for item in test_labels ]\n",
        "test_labels = np.asarray(test_labels, dtype=np.int32)\n",
        "test_label_one_hot = tf.keras.utils.to_categorical(test_labels, dtype='int32')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcBFNuK1FKJm",
        "outputId": "6036396e-dd05-4927-f0df-fd33c4fd21d5"
      },
      "source": [
        "# Create model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size+1, output_dim=32,\n",
        "                    input_length=maxlen, mask_zero=True))\n",
        "# vocab_size+1 is to inlcude the padding\n",
        "# output_dim is the size of the embedding for the characters in the vocab\n",
        "model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
        "#model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
        "# 5 is the number of labels and which we need in the softamx classifier\n",
        "model.add(Dense(len(labels_dict), activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 32)           2400      \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 64)                16640     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 19,365\n",
            "Trainable params: 19,365\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCOwOI99FSVq"
      },
      "source": [
        "# create checkpoint\n",
        "checkpoint_filepath = 'model_checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    save_best_only=False,\n",
        "    monitor='val_accuracy',\n",
        "    save_freq='epoch',\n",
        "    mode='max',\n",
        "    verbose=0,\n",
        "    options=None\n",
        ")\n",
        "adam = keras.optimizers.Adam(learning_rate=0.001015)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_padded, train_label_one_hot,batch_size=32, epochs=10,  # TRAIN THE MODEL\n",
        "                    validation_data=(test_padded, test_label_one_hot), callbacks=[model_checkpoint_callback]\n",
        "                    )\n",
        "model.save('model/intelligent-address-parser-model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWVbup2dDG_A"
      },
      "source": [
        "# Entity Disambiguation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eElXBELguc3c"
      },
      "source": [
        "# load the english spacy model\n",
        "# this is used to get vectors for text\n",
        "nlp = en_core_web_lg.load()"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehXXQGPs9-5W"
      },
      "source": [
        "# setting up the tables where data will be stored\n",
        "company_name_entity_table = pd.DataFrame({'entity':'','group_id':int()},index=[])\n",
        "company_address_entity_table = pd.DataFrame({'entity':'','group_id':int()},index=[])"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "vq25DgXeGtM5",
        "outputId": "52710541-90e6-41d7-cef6-26a1bb12ca7c"
      },
      "source": [
        "# sample serial numbers inital values\n",
        "serial_number_entity_table = pd.DataFrame({'entity':'','group_id':int()},index=[])\n",
        "\n",
        "serial_number_entity_table=pd.DataFrame({'entity':['abc12345','bhkdj9849204','zlljdoi9720483','bbnvnv909090'],\n",
        "                                   'group_id':[0,1,2,3]})\n",
        "serial_number_entity_table"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>entity</th>\n",
              "      <th>group_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>abc12345</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bhkdj9849204</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>zlljdoi9720483</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bbnvnv909090</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           entity  group_id\n",
              "0        abc12345         0\n",
              "1    bhkdj9849204         1\n",
              "2  zlljdoi9720483         2\n",
              "3    bbnvnv909090         3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "qD1-aJ_Bol-3",
        "outputId": "ba9c6353-1c5a-402f-8f3e-58631e5c3b03"
      },
      "source": [
        "# sample physical goods initial values\n",
        "physical_good_entity_table = pd.DataFrame({'entity':'','group_id':int()},index=[])\n",
        "\n",
        "physical_good_entity_table=physical_good_entity_table.append({'entity':'plastic bottle','group_id':0,'vector':nlp('plastic bottle').vector},ignore_index=True)\n",
        "physical_good_entity_table=physical_good_entity_table.append({'entity':'steel bowl','group_id':1,'vector':nlp('steel bowl').vector},ignore_index=True)\n",
        "physical_good_entity_table=physical_good_entity_table.append({'entity':'leather sofa','group_id':2,'vector':nlp('leather sofa').vector},ignore_index=True)\n",
        "physical_good_entity_table=physical_good_entity_table.append({'entity':'hardwood table','group_id':3,'vector':nlp('hardwood table').vector},ignore_index=True)\n",
        "\n",
        "physical_good_entity_table"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>entity</th>\n",
              "      <th>group_id</th>\n",
              "      <th>vector</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>plastic bottle</td>\n",
              "      <td>0</td>\n",
              "      <td>[-0.289565, -0.049595, -0.077429, -0.15209301,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>steel bowl</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.12778, 0.25831202, 0.45532, -0.372285, -0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>leather sofa</td>\n",
              "      <td>2</td>\n",
              "      <td>[-0.06037, -0.40972, -0.208975, -0.005081499, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hardwood table</td>\n",
              "      <td>3</td>\n",
              "      <td>[-0.091709, 0.09808999, -0.29916, -0.36364597,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           entity  group_id                                             vector\n",
              "0  plastic bottle         0  [-0.289565, -0.049595, -0.077429, -0.15209301,...\n",
              "1      steel bowl         1  [0.12778, 0.25831202, 0.45532, -0.372285, -0.1...\n",
              "2    leather sofa         2  [-0.06037, -0.40972, -0.208975, -0.005081499, ...\n",
              "3  hardwood table         3  [-0.091709, 0.09808999, -0.29916, -0.36364597,..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "LIe3en6btXiy",
        "outputId": "9beb3f12-1e97-4838-dda6-81baff5b7b23"
      },
      "source": [
        "# sample location initial value\n",
        "location_entity_table = pd.DataFrame({'entity':'','group_id':int()},index=[])\n",
        "\n",
        "location_entity_table=location_entity_table.append({'entity':'London','group_id':0,'vector':nlp('London, UK').vector},ignore_index=True)\n",
        "location_entity_table=location_entity_table.append({'entity':'Rome, Italy','group_id':1,'vector':nlp('Rome, Italy').vector},ignore_index=True)\n",
        "location_entity_table=location_entity_table.append({'entity':'Tokyo','group_id':2,'vector':nlp('Tokyo').vector},ignore_index=True)\n",
        "location_entity_table=location_entity_table.append({'entity':'Japan','group_id':3,'vector':nlp('Japan').vector},ignore_index=True)\n",
        "\n",
        "location_entity_table"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>entity</th>\n",
              "      <th>group_id</th>\n",
              "      <th>vector</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>London</td>\n",
              "      <td>0</td>\n",
              "      <td>[-0.040734004, 0.24897666, 0.082936674, -0.140...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Rome, Italy</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.041836005, 0.30547366, -0.11563143, -0.2124...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tokyo</td>\n",
              "      <td>2</td>\n",
              "      <td>[0.28876, -0.55541, 0.083178, -0.19359, 0.3757...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Japan</td>\n",
              "      <td>3</td>\n",
              "      <td>[-0.44528, -0.17553, 0.075346, 0.0048481, 0.23...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        entity  group_id                                             vector\n",
              "0       London         0  [-0.040734004, 0.24897666, 0.082936674, -0.140...\n",
              "1  Rome, Italy         1  [0.041836005, 0.30547366, -0.11563143, -0.2124...\n",
              "2        Tokyo         2  [0.28876, -0.55541, 0.083178, -0.19359, 0.3757...\n",
              "3        Japan         3  [-0.44528, -0.17553, 0.075346, 0.0048481, 0.23..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YnKifKQjCeY"
      },
      "source": [
        "# serial_grouper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XVmZlpg-sQH"
      },
      "source": [
        "#decide to group or to add as new entity\n",
        "#from rapidfuzz import fuzz\n",
        "\n",
        "def serial_grouper(input_string):\n",
        "    global serial_number_entity_table\n",
        "    #input_string=sentence\n",
        "    grouped_flag=0\n",
        "    for index, row in serial_number_entity_table.iterrows():\n",
        "\n",
        "        if re.findall(\"\\A\"+input_string[0:3], row['entity']): # check if the starting 3 characters are same, if yes then group them\n",
        "            # save the entity in table and assign the same group id\n",
        "            serial_number_entity_table=serial_number_entity_table.append({'entity':input_string, 'group_id':row['group_id']},ignore_index=True)\n",
        "            print('entity grouped: ',{'entity':input_string, 'group_id':row['group_id']})\n",
        "            grouped_flag=1\n",
        "            break\n",
        "        \n",
        "\n",
        "    if grouped_flag==0:\n",
        "        # save it as a new entity\n",
        "        last=max(list(serial_number_entity_table['group_id']))\n",
        "        serial_number_entity_table=serial_number_entity_table.append({'entity':input_string,'group_id':last+1},ignore_index=True)\n",
        "        print('entity saved as new entry in the table: ', {'entity':input_string,'group_id':last+1})  \n",
        "        \n",
        "    return"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tEd9z1Yi6xj"
      },
      "source": [
        "#goods_grouper\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPNUp8hHxx1i"
      },
      "source": [
        "# importing spelling corrector, we can easily use spelling correction as physical goods are very common and new names dont come in this\n",
        "spell = Speller(lang='en')"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrL8KHWGl7Qc"
      },
      "source": [
        "def goods_grouper(input_entity):\n",
        "    global physical_good_entity_table\n",
        "    #input_entity='chair'\n",
        "    input_entity=spell(input_entity) # spelling correction # physical goods have very general name and so we can easily apply spelling correction\n",
        "\n",
        "    input_vector=nlp(input_entity).vector\n",
        "    list_of_similarities=[]\n",
        "    for index, row in physical_good_entity_table.iterrows():\n",
        "        result = 1 - spatial.distance.cosine(row['vector'], input_vector )\n",
        "        print(result)\n",
        "        list_of_similarities.append(result)\n",
        "\n",
        "    max_sim=max(list_of_similarities)\n",
        "    print('max sim:',max_sim)\n",
        "    if max_sim>0.72:\n",
        "        group_id=list_of_similarities.index(max_sim)\n",
        "        physical_good_entity_table=physical_good_entity_table.append({'entity':input_entity,'group_id':group_id,'vector':input_vector},ignore_index=True)\n",
        "        print('entity grouped: ',{'entity':input_entity,'group_id':group_id})\n",
        "\n",
        "    else:\n",
        "        last=max(list(physical_good_entity_table['group_id']))\n",
        "        physical_good_entity_table=physical_good_entity_table.append({'entity':input_entity,'group_id':last+1,'vector':input_vector},ignore_index=True)\n",
        "        print('entity added: ',{'entity':input_entity,'group_id':last+1})\n"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHSjO6aoB-UP"
      },
      "source": [
        "# location_grouper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO2GazHZPddU"
      },
      "source": [
        "vectors are helpful in reducing the search space, or you can say it helps to get candidates then we do string comparisions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdQBGVm2Ck7w"
      },
      "source": [
        "#spell = Speller(lang='en') # spelling correction was not working properly over here\n",
        "\n",
        "def location_grouper(input_entity):\n",
        "    global location_entity_table\n",
        "    #input_entity='London, UK'\n",
        "    #input_entity=spell(input_entity) # spelling correction # physical goods have very general name and so we can easily apply spelling correction\n",
        "\n",
        "    input_vector=nlp(input_entity).vector\n",
        "    list_of_similarities=[]\n",
        "    candidates=[]\n",
        "    for index, row in location_entity_table.iterrows():\n",
        "        result = 1 - spatial.distance.cosine(row['vector'], input_vector )\n",
        "        print(result)\n",
        "        if result>0.70:\n",
        "            candidates.append(index)\n",
        "        #list_of_similarities.append(result)\n",
        "\n",
        "    print('candidates:',candidates)\n",
        "\n",
        "    if candidates:\n",
        "        # prep input_entity for comparision by splitting it at spaces and\n",
        "        input_entity_modified = input_entity.split(',')\n",
        "        print('input_entity_modified',input_entity_modified)\n",
        "        in_vec_list=[] # list of individual vectors for the entity parts\n",
        "        for item in input_entity_modified:\n",
        "            in_vec_list.append(nlp(item.strip()).vector)\n",
        "            \n",
        "\n",
        "        # for each candidate , replace comma with space and split at space, and then compare the vectors\n",
        "\n",
        "        candidate_score_list=[]\n",
        "        for item in candidates :\n",
        "            can_string=location_entity_table['entity'][item] # replace comma with space\n",
        "            can_string=can_string.split(',')\n",
        "\n",
        "            print('can_string: ',can_string)\n",
        "            can_vec_list=[] # list of individual vectors for the entity parts\n",
        "            for i in can_string:\n",
        "                can_vec_list.append(nlp(i.strip()).vector)\n",
        "\n",
        "            # now we compare the elements from the 2 vector lists\n",
        "            print('in_vec_list length',len(in_vec_list))\n",
        "            print('can_vec_list length',len(can_vec_list))\n",
        "\n",
        "            if len(in_vec_list)==1 and len(can_vec_list)==1:\n",
        "                candidate_score_list.append(1 - spatial.distance.cosine(can_vec_list[0], in_vec_list[0]))\n",
        "\n",
        "            elif len(in_vec_list)==1 and len(can_vec_list)==2:# first part is important\n",
        "                candidate_score_list.append(1 - spatial.distance.cosine(can_vec_list[0], in_vec_list[0]))\n",
        "\n",
        "            elif len(in_vec_list)==2 and len(can_vec_list)==1: # first part is important\n",
        "                print('executing : in2 and can1')\n",
        "                candidate_score_list.append(1 - spatial.distance.cosine(can_vec_list[0], in_vec_list[0]))\n",
        "\n",
        "            elif len(in_vec_list)==2 and len(can_vec_list)==2: # least score wil determine \n",
        "                candidate_score_list.append(min([1 - spatial.distance.cosine(can_vec_list[0], in_vec_list[0]), 1 - spatial.distance.cosine(can_vec_list[1], in_vec_list[1])]))\n",
        "\n",
        "            else:\n",
        "                print('no criteria executed')\n",
        "\n",
        "\n",
        "        print('candidate_score_list: ',candidate_score_list)\n",
        "        max_sim=max(candidate_score_list)\n",
        "\n",
        "        print('max sim:',max_sim)\n",
        "        if max_sim>0.85:\n",
        "            group_id=location_entity_table['group_id'][candidates[candidate_score_list.index(max_sim)]]\n",
        "            location_entity_table=location_entity_table.append({'entity':input_entity,'group_id':group_id,'vector':input_vector},ignore_index=True)\n",
        "            print('entity grouped: ',{'entity':input_entity,'group_id':group_id})\n",
        "\n",
        "        else:\n",
        "            last=max(list(location_entity_table['group_id']))\n",
        "            location_entity_table=location_entity_table.append({'entity':input_entity,'group_id':last+1,'vector':input_vector},ignore_index=True)\n",
        "            print('entity added: ',{'entity':input_entity,'group_id':last+1})\n",
        "\n",
        "    else:\n",
        "        last=max(list(location_entity_table['group_id']))\n",
        "        location_entity_table=location_entity_table.append({'entity':input_entity,'group_id':last+1,'vector':input_vector},ignore_index=True)\n",
        "        print('entity added: ',{'entity':input_entity,'group_id':last+1})"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY9XNU9h0sNL"
      },
      "source": [
        "# classifier_main_function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LToRo6WV0g8Y"
      },
      "source": [
        "def classifier_main_function():\n",
        "    global model\n",
        "    global tokenizer\n",
        "    global serial_number_entity_table\n",
        "    global physical_good_entity_table\n",
        "    global location_entity_table\n",
        "    # prediction (this is the starting point of our full model)\n",
        "\n",
        "    input_entity=input('enter entity')\n",
        "    sentence = [input_entity] # convert into list\n",
        "    sequences = tokenizer.texts_to_sequences(sentence)\n",
        "    padded = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='pre')\n",
        "    output_class=reversed_labels[np.argmax(model.predict(padded)[0], axis=-1)]\n",
        "    print('the input entity is classified as: ', output_class)\n",
        "\n",
        "\n",
        "    if output_class=='serial_number':\n",
        "        serial_grouper(input_entity)\n",
        "        print('-'*60)\n",
        "        print(serial_number_entity_table)\n",
        "\n",
        "    elif output_class=='physical_good':\n",
        "        goods_grouper(input_entity)\n",
        "        print('-'*60)\n",
        "        print(physical_good_entity_table)\n",
        "\n",
        "    elif output_class=='location':\n",
        "        location_grouper(input_entity)\n",
        "        print('-'*60)\n",
        "        print(location_entity_table)\n",
        "\n",
        "    return\n",
        "    "
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08A2Z8lQ2VDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf1d8ac-2fae-4b8a-d034-8d62cb7366a0"
      },
      "source": [
        "# calling the master function\n",
        "classifier_main_function()"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "enter entityLondon, UK\n",
            "the input entity is classified as:  location\n",
            "1.0\n",
            "0.6525508165359497\n",
            "0.45524969696998596\n",
            "0.5104739665985107\n",
            "candidates: [0]\n",
            "input_entity_modified ['London', ' UK']\n",
            "can_string:  ['London']\n",
            "in_vec_list length 2\n",
            "can_vec_list length 1\n",
            "executing : in2 and can1\n",
            "candidate_score_list:  [1.0]\n",
            "max sim: 1.0\n",
            "entity grouped:  {'entity': 'London, UK', 'group_id': 0}\n",
            "------------------------------------------------------------\n",
            "        entity  group_id                                             vector\n",
            "0       London         0  [-0.040734004, 0.24897666, 0.082936674, -0.140...\n",
            "1  Rome, Italy         1  [0.041836005, 0.30547366, -0.11563143, -0.2124...\n",
            "2        Tokyo         2  [0.28876, -0.55541, 0.083178, -0.19359, 0.3757...\n",
            "3        Japan         3  [-0.44528, -0.17553, 0.075346, 0.0048481, 0.23...\n",
            "4   London, UK         0  [-0.040734004, 0.24897666, 0.082936674, -0.140...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6syueSpf_eM1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}