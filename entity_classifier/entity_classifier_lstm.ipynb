{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "entity_classifier_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.4-final"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo7CtpewFt51"
      },
      "source": [
        "Description: \n",
        "The full model consists of a character level Bi-lstm classifier which classifies the input entitiy into one of 5 categories, for each category I have a separate logic for grouping and storing the entitiy.\n",
        "\n",
        "Grouping logic:\n",
        "\n",
        "1. Serial numbers: instead of using edit distance I have just matched the first 3 characters, if they are equal the serial numbers are grouped together (Serial numbers which have starting part same would be a closer match).\n",
        "2. Physical goods: used embeddings for the words from \"en_core_web_lg\" spacy model and calculated cosine simialrity to find the best match. Also added a spelling corrector as it is easy to correct spellings in this case.\n",
        "3. Locations: used embedding of the entire string initially to find candidates, then used embeddings for each word separately and calculated cosine simialrity to find the best match.\n",
        "4. Company names: used Wikidata to retrieve all the aliases for a company and used fuzzy matching to find the best match. Library used for wikidata is not robust to name variations, so I have also used google search engine api for making it more tolerant to name variations. Once we have all the aliases for a company it becomes very likely that we will be able to match the names which occur in the input.\n",
        "5. Addresses: For this I have used an address parser which breaks the address into its constituent parts, after which I used fuzzy matching to find the best match.\n",
        "\n",
        "The master function that you will be calling is `classifier_main_function`.\n",
        "You can just give the command to run all the cells to set up everything for a quick try.\n",
        "\n",
        "note: \n",
        "\n",
        "for fuzzy matching I am using `rapidfuzz` library which uses Levenshtein distance.\n",
        "\n",
        "`datafile_lstm.csv` contains the small manually annotated training data used to train the classifier(around 500 rows, 100 each category).\n",
        "\n",
        "`intelligent-address-parser-model-40mn.h5` contains a trained model used for address parsing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3ktt5nmDCEq"
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install pywikibot\n",
        "!pip install jellyfish\n",
        "!pip install rapidfuzz\n",
        "!pip install autocorrect\n",
        "!pip install spacy==2.2.4\n"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /home/priyank/anaconda3/lib/python3.7/site-packages (2.4.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.15.7)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: gast==0.3.3 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (41.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (2.25.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.28.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/priyank/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.1.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /home/priyank/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (0.23)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/priyank/anaconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.26.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/priyank/anaconda3/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: more-itertools in /home/priyank/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (7.2.0)\n",
            "Requirement already satisfied: pywikibot in /home/priyank/anaconda3/lib/python3.7/site-packages (6.0.1)\n",
            "Requirement already satisfied: requests>=2.20.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from pywikibot) (2.25.0)\n",
            "Requirement already satisfied: setuptools>=20.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from pywikibot) (41.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests>=2.20.1->pywikibot) (2019.9.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests>=2.20.1->pywikibot) (2.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests>=2.20.1->pywikibot) (1.26.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests>=2.20.1->pywikibot) (3.0.4)\n",
            "Requirement already satisfied: jellyfish in /home/priyank/anaconda3/lib/python3.7/site-packages (0.8.2)\n",
            "Requirement already satisfied: rapidfuzz in /home/priyank/anaconda3/lib/python3.7/site-packages (0.13.3)\n",
            "Requirement already satisfied: autocorrect in /home/priyank/anaconda3/lib/python3.7/site-packages (1.1.0)\n",
            "Requirement already satisfied: spacy==2.2.4 in /home/priyank/anaconda3/lib/python3.7/site-packages (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (2.25.0)\n",
            "Requirement already satisfied: setuptools in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (41.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (4.55.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /home/priyank/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (0.23)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/priyank/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (0.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (1.26.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2.8)\n",
            "Requirement already satisfied: more-itertools in /home/priyank/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (7.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 29.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (41.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.55.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.25.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /home/priyank/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (0.23)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/priyank/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.26.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: more-itertools in /home/priyank/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (7.2.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /home/priyank/anaconda3/lib/python3.7/site-packages (2.4.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.15.7)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: six~=1.15.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (41.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.28.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (2.25.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.1.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/priyank/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /home/priyank/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (0.23)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/priyank/anaconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.26.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/priyank/anaconda3/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: more-itertools in /home/priyank/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (7.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2PakEB-nUln"
      },
      "source": [
        "import pywikibot\n",
        "from rapidfuzz import fuzz \n",
        "from pprint import pprint\n",
        "import en_core_web_lg\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "\n",
        "from scipy import spatial\n",
        "from autocorrect import Speller\n",
        "\n",
        "import re\n",
        "\n",
        "from googlesearch import search\n"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSY7fMTgDwTp"
      },
      "source": [
        "import string\n",
        "from random import randrange\n",
        "import random  # used to create random separator\n",
        "import string  # this is used to create the vocab of all the characters , stored in the variable 'vocab'\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "import csv\n",
        "import pickle\n",
        "\n",
        "# below packages are used to create the model\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Bidirectional, TimeDistributed, Input, Masking\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n070HCcGQBZe"
      },
      "source": [
        "# max length of the input\n",
        "maxlen=100"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9v4nP5R5mPa"
      },
      "source": [
        "# load the csv data file\n",
        "df=pd.read_csv('datafile_lstm.csv')"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1c5g1hyRg6V"
      },
      "source": [
        "# shuffle data frame\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl0w9Aj12BYB"
      },
      "source": [
        "# split the dataframe into train and test\n",
        "df1=df.iloc[100:500,:]\n",
        "df2=df.iloc[:100,:]"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv2_qn6e13xF"
      },
      "source": [
        "# Dictionary contains the Labels\n",
        "labels_dict = {  # we are keeping the value part as strings as it easy to generate labels , we could just give len(string)*label value to generate the label\n",
        "    'company_name': 0,\n",
        "    'company_address': 1,\n",
        "    'serial_number': 2,\n",
        "    'physical_good': 3,\n",
        "    'location': 4\n",
        "}"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBGS7WJP18UR",
        "outputId": "2b0e5d7e-398a-4efa-ec2d-7970b3856065"
      },
      "source": [
        "# create reverse dictionary of label placeholders\n",
        "reversed_labels = {value: key for (key, value) in labels_dict.items()}\n",
        "reversed_labels"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'company_name',\n",
              " 1: 'company_address',\n",
              " 2: 'serial_number',\n",
              " 3: 'physical_good',\n",
              " 4: 'location'}"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2LvIHDsD50q"
      },
      "source": [
        "# define the vocab\n",
        "vocab = list(string.whitespace + string.digits +\n",
        "             string.ascii_lowercase + string.punctuation)  # total 74 characters"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxqdzJTkPzZH",
        "outputId": "8a304f34-7795-4bac-c8d1-4426f038fb24"
      },
      "source": [
        "vocab_size=len(vocab)\n",
        "vocab_size"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74"
            ]
          },
          "metadata": {},
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTje8-NnEJE1"
      },
      "source": [
        "# create the Tokenizer object, word_index contaning all the chars in the vocab\n",
        "# basically this will encode all the characters into numbers, so each character defined in our vocab will be represented by a number\n",
        "# vocab is just a list of tokens\n",
        "tokenizer = Tokenizer(num_words=len(vocab), filters='',\n",
        "                      char_level=True, oov_token='<oov>')\n",
        "tokenizer.fit_on_texts(vocab)"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSHrIukISxdE"
      },
      "source": [
        "#train\n",
        "train_sentences=list(df1['text'])\n",
        "train_labels=list(df1['category'])\n",
        "train_sequences=tokenizer.texts_to_sequences(train_sentences)\n",
        "train_padded=pad_sequences(train_sequences, padding='post',maxlen=maxlen, truncating='pre') #maxlen decides the maximum length of the sequence\n",
        "train_labels=[labels_dict[item] for item in train_labels ]\n",
        "train_labels = np.asarray(train_labels, dtype=np.int32)\n",
        "train_label_one_hot = tf.keras.utils.to_categorical(train_labels, dtype='int32')\n"
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRmyMQFhR6Hz"
      },
      "source": [
        "#test\n",
        "test_sentences=list(df2['text'])\n",
        "test_labels=list(df2['category'])\n",
        "test_sequences=tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded=pad_sequences(test_sequences, padding='post',maxlen=maxlen, truncating='pre') #maxlen decides the maximum length of the sequence\n",
        "test_labels=[labels_dict[item] for item in test_labels ]\n",
        "test_labels = np.asarray(test_labels, dtype=np.int32)\n",
        "test_label_one_hot = tf.keras.utils.to_categorical(test_labels, dtype='int32')\n"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcBFNuK1FKJm",
        "outputId": "6036396e-dd05-4927-f0df-fd33c4fd21d5"
      },
      "source": [
        "# Create model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size+1, output_dim=32,\n",
        "                    input_length=maxlen, mask_zero=True))\n",
        "# vocab_size+1 is to inlcude the padding\n",
        "# output_dim is the size of the embedding for the characters in the vocab\n",
        "model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
        "#model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
        "# 5 is the number of labels and which we need in the softamx classifier\n",
        "model.add(Dense(len(labels_dict), activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 100, 32)           2400      \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 64)                16640     \n_________________________________________________________________\ndense_1 (Dense)              (None, 5)                 325       \n=================================================================\nTotal params: 19,365\nTrainable params: 19,365\nNon-trainable params: 0\n_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCOwOI99FSVq"
      },
      "source": [
        "# create checkpoint\n",
        "checkpoint_filepath = 'model_checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    save_best_only=False,\n",
        "    monitor='val_accuracy',\n",
        "    save_freq='epoch',\n",
        "    mode='max',\n",
        "    verbose=0,\n",
        "    options=None\n",
        ")\n",
        "adam = keras.optimizers.Adam(learning_rate=0.001015)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_padded, train_label_one_hot,batch_size=32, epochs=10,  # TRAIN THE MODEL\n",
        "                    validation_data=(test_padded, test_label_one_hot)#, callbacks=[model_checkpoint_callback]\n",
        "                    )\n",
        "model.save('model/intelligent-address-parser-model.h5')"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "13/13 [==============================] - 12s 288ms/step - loss: 1.6051 - accuracy: 0.3223 - val_loss: 1.5828 - val_accuracy: 0.7600\n",
            "Epoch 2/10\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 1.5686 - accuracy: 0.7162 - val_loss: 1.4942 - val_accuracy: 0.6100\n",
            "Epoch 3/10\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 1.4380 - accuracy: 0.5627 - val_loss: 1.0707 - val_accuracy: 0.5200\n",
            "Epoch 4/10\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 1.0882 - accuracy: 0.5415 - val_loss: 0.8234 - val_accuracy: 0.6200\n",
            "Epoch 5/10\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.7954 - accuracy: 0.6177 - val_loss: 0.6994 - val_accuracy: 0.6600\n",
            "Epoch 6/10\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 0.7094 - accuracy: 0.6985 - val_loss: 0.6083 - val_accuracy: 0.7500\n",
            "Epoch 7/10\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.6465 - accuracy: 0.7516 - val_loss: 0.5675 - val_accuracy: 0.7900\n",
            "Epoch 8/10\n",
            "13/13 [==============================] - 1s 94ms/step - loss: 0.5561 - accuracy: 0.8229 - val_loss: 0.5051 - val_accuracy: 0.8700\n",
            "Epoch 9/10\n",
            "13/13 [==============================] - 1s 91ms/step - loss: 0.5037 - accuracy: 0.8534 - val_loss: 0.4484 - val_accuracy: 0.8600\n",
            "Epoch 10/10\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.4067 - accuracy: 0.8810 - val_loss: 0.4161 - val_accuracy: 0.8600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWVbup2dDG_A"
      },
      "source": [
        "# Entity Disambiguation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eElXBELguc3c"
      },
      "source": [
        "# load the english spacy model\n",
        "# this is used to get vectors for text\n",
        "nlp = en_core_web_lg.load()"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YnKifKQjCeY"
      },
      "source": [
        "# serial_grouper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XVmZlpg-sQH"
      },
      "source": [
        "#decide to group or to add as new entity\n",
        "#from rapidfuzz import fuzz\n",
        "\n",
        "def serial_grouper(input_string):\n",
        "    global serial_number_entity_table\n",
        "    #input_string=sentence\n",
        "    grouped_flag=0\n",
        "    for index, row in serial_number_entity_table.iterrows():\n",
        "\n",
        "        if re.findall(\"\\A\"+input_string[0:3], row['entity']): # check if the starting 3 characters are same, if yes then group them\n",
        "            # save the entity in table and assign the same group id\n",
        "            serial_number_entity_table=serial_number_entity_table.append({'entity':input_string, 'group_id':row['group_id']},ignore_index=True)\n",
        "            print('entity grouped: ',{'entity':input_string, 'group_id':row['group_id']})\n",
        "            grouped_flag=1\n",
        "            break\n",
        "        \n",
        "\n",
        "    if grouped_flag==0:\n",
        "        # save it as a new entity\n",
        "        last=max(list(serial_number_entity_table['group_id']))\n",
        "        serial_number_entity_table=serial_number_entity_table.append({'entity':input_string,'group_id':last+1},ignore_index=True)\n",
        "        print('entity saved as new entry in the table: ', {'entity':input_string,'group_id':last+1})  \n",
        "        \n",
        "    return"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           entity  group_id\n",
              "0        abc12345         0\n",
              "1    bhkdj9849204         1\n",
              "2  zlljdoi9720483         2\n",
              "3    bbnvnv909090         3"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>entity</th>\n      <th>group_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>abc12345</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>bhkdj9849204</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>zlljdoi9720483</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>bbnvnv909090</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 236
        }
      ],
      "source": [
        "# sample serial numbers inital values\n",
        "serial_number_entity_table = pd.DataFrame({'entity':'','group_id':int()},index=[])\n",
        "\n",
        "serial_number_entity_table=pd.DataFrame({'entity':['abc12345','bhkdj9849204','zlljdoi9720483','bbnvnv909090'],\n",
        "                                   'group_id':[0,1,2,3]})\n",
        "serial_number_entity_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tEd9z1Yi6xj"
      },
      "source": [
        "# goods_grouper\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPNUp8hHxx1i"
      },
      "source": [
        "# importing spelling corrector, we can easily use spelling correction as physical goods are very common and new names dont come in this\n",
        "spell = Speller(lang='en')"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrL8KHWGl7Qc"
      },
      "source": [
        "def goods_grouper(input_entity):\n",
        "    global physical_good_entity_table\n",
        "    #input_entity='chair'\n",
        "    input_entity=spell(input_entity) # spelling correction # physical goods have very general name and so we can easily apply spelling correction\n",
        "\n",
        "    input_vector=nlp(input_entity).vector\n",
        "    list_of_similarities=[]\n",
        "    for index, row in physical_good_entity_table.iterrows():\n",
        "        result = 1 - spatial.distance.cosine(row['vector'], input_vector )\n",
        "        print(result)\n",
        "        list_of_similarities.append(result)\n",
        "\n",
        "    max_sim=max(list_of_similarities)\n",
        "    print('max sim:',max_sim)\n",
        "    if max_sim>=0.72:\n",
        "        group_id=physical_good_entity_table['group_id'][list_of_similarities.index(max_sim)]\n",
        "        physical_good_entity_table=physical_good_entity_table.append({'entity':input_entity,'group_id':group_id,'vector':input_vector},ignore_index=True)\n",
        "        print('entity grouped: ',{'entity':input_entity,'group_id':group_id})\n",
        "\n",
        "    else:\n",
        "        last=max(list(physical_good_entity_table['group_id']))\n",
        "        print('last',last)\n",
        "        physical_good_entity_table=physical_good_entity_table.append({'entity':input_entity,'group_id':last+1,'vector':input_vector},ignore_index=True)\n",
        "        print('new entity added: ',{'entity':input_entity,'group_id':last+1})\n",
        "\n",
        "\n",
        "    return"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           entity  group_id                                             vector\n",
              "0  plastic bottle         0  [-0.289565, -0.049595, -0.077429, -0.15209301,...\n",
              "1      steel bowl         1  [0.12778, 0.25831202, 0.45532, -0.372285, -0.1...\n",
              "2    leather sofa         2  [-0.06037, -0.40972, -0.208975, -0.005081499, ...\n",
              "3  hardwood table         3  [-0.091709, 0.09808999, -0.29916, -0.36364597,..."
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>entity</th>\n      <th>group_id</th>\n      <th>vector</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>plastic bottle</td>\n      <td>0</td>\n      <td>[-0.289565, -0.049595, -0.077429, -0.15209301,...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>steel bowl</td>\n      <td>1</td>\n      <td>[0.12778, 0.25831202, 0.45532, -0.372285, -0.1...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>leather sofa</td>\n      <td>2</td>\n      <td>[-0.06037, -0.40972, -0.208975, -0.005081499, ...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>hardwood table</td>\n      <td>3</td>\n      <td>[-0.091709, 0.09808999, -0.29916, -0.36364597,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 239
        }
      ],
      "source": [
        "# sample physical goods initial values\n",
        "physical_good_entity_table = pd.DataFrame({'entity':'','group_id':int()},index=[])\n",
        "\n",
        "physical_good_entity_table=physical_good_entity_table.append({'entity':'plastic bottle','group_id':0,'vector':nlp('plastic bottle').vector},ignore_index=True)\n",
        "physical_good_entity_table=physical_good_entity_table.append({'entity':'steel bowl','group_id':1,'vector':nlp('steel bowl').vector},ignore_index=True)\n",
        "physical_good_entity_table=physical_good_entity_table.append({'entity':'leather sofa','group_id':2,'vector':nlp('leather sofa').vector},ignore_index=True)\n",
        "physical_good_entity_table=physical_good_entity_table.append({'entity':'hardwood table','group_id':3,'vector':nlp('hardwood table').vector},ignore_index=True)\n",
        "\n",
        "physical_good_entity_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHSjO6aoB-UP"
      },
      "source": [
        "# location_grouper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdQBGVm2Ck7w"
      },
      "source": [
        "#spell = Speller(lang='en') # spelling correction was not working properly over here\n",
        "\n",
        "def location_grouper(input_entity):\n",
        "    global location_entity_table\n",
        "    #input_entity='London, UK'\n",
        "    #input_entity=spell(input_entity) # spelling correction # physical goods have very general name and so we can easily apply spelling correction\n",
        "\n",
        "    input_vector=nlp(input_entity).vector\n",
        "    list_of_similarities=[]\n",
        "    candidates=[]\n",
        "    for index, row in location_entity_table.iterrows():\n",
        "        result = 1 - spatial.distance.cosine(row['vector'], input_vector )\n",
        "        print(result)\n",
        "        if result>0.70:\n",
        "            candidates.append(index)\n",
        "        #list_of_similarities.append(result)\n",
        "\n",
        "    print('candidates:',candidates)\n",
        "\n",
        "    if candidates:\n",
        "        # prep input_entity for comparision by splitting it at spaces and\n",
        "        input_entity_modified = input_entity.split(',')\n",
        "        print('input_entity_modified',input_entity_modified)\n",
        "        in_vec_list=[] # list of individual vectors for the entity parts\n",
        "        for item in input_entity_modified:\n",
        "            in_vec_list.append(nlp(item.strip()).vector)\n",
        "            \n",
        "\n",
        "        # for each candidate , replace comma with space and split at space, and then compare the vectors\n",
        "\n",
        "        candidate_score_list=[]\n",
        "        for item in candidates :\n",
        "            can_string=location_entity_table['entity'][item] # replace comma with space\n",
        "            can_string=can_string.split(',')\n",
        "\n",
        "            print('can_string: ',can_string)\n",
        "            can_vec_list=[] # list of individual vectors for the entity parts\n",
        "            for i in can_string:\n",
        "                can_vec_list.append(nlp(i.strip()).vector)\n",
        "\n",
        "            # now we compare the elements from the 2 vector lists\n",
        "            print('in_vec_list length',len(in_vec_list))\n",
        "            print('can_vec_list length',len(can_vec_list))\n",
        "\n",
        "            if len(in_vec_list)==1 and len(can_vec_list)==1:\n",
        "                candidate_score_list.append(1 - spatial.distance.cosine(can_vec_list[0], in_vec_list[0]))\n",
        "\n",
        "            elif len(in_vec_list)==1 and len(can_vec_list)==2:# first part is important\n",
        "                candidate_score_list.append(1 - spatial.distance.cosine(can_vec_list[0], in_vec_list[0]))\n",
        "\n",
        "            elif len(in_vec_list)==2 and len(can_vec_list)==1: # first part is important\n",
        "                print('executing : in2 and can1')\n",
        "                candidate_score_list.append(1 - spatial.distance.cosine(can_vec_list[0], in_vec_list[0]))\n",
        "\n",
        "            elif len(in_vec_list)==2 and len(can_vec_list)==2: # least score wil determine \n",
        "                candidate_score_list.append(min([1 - spatial.distance.cosine(can_vec_list[0], in_vec_list[0]), 1 - spatial.distance.cosine(can_vec_list[1], in_vec_list[1])]))\n",
        "\n",
        "            else:\n",
        "                print('no criteria executed')\n",
        "\n",
        "\n",
        "        print('candidate_score_list: ',candidate_score_list)\n",
        "        max_sim=max(candidate_score_list)\n",
        "\n",
        "        print('max sim:',max_sim)\n",
        "        if max_sim>0.85:\n",
        "            group_id=location_entity_table['group_id'][candidates[candidate_score_list.index(max_sim)]]\n",
        "            location_entity_table=location_entity_table.append({'entity':input_entity,'group_id':group_id,'vector':input_vector},ignore_index=True)\n",
        "            print('entity grouped: ',{'entity':input_entity,'group_id':group_id})\n",
        "\n",
        "        else:\n",
        "            last=max(list(location_entity_table['group_id']))\n",
        "            location_entity_table=location_entity_table.append({'entity':input_entity,'group_id':last+1,'vector':input_vector},ignore_index=True)\n",
        "            print('entity added: ',{'entity':input_entity,'group_id':last+1})\n",
        "\n",
        "    else:\n",
        "        last=max(list(location_entity_table['group_id']))\n",
        "        location_entity_table=location_entity_table.append({'entity':input_entity,'group_id':last+1,'vector':input_vector},ignore_index=True)\n",
        "        print('entity added: ',{'entity':input_entity,'group_id':last+1})"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        entity  group_id                                             vector\n",
              "0       London         0  [-0.040734004, 0.24897666, 0.082936674, -0.140...\n",
              "1  Rome, Italy         1  [0.041836005, 0.30547366, -0.11563143, -0.2124...\n",
              "2        Tokyo         2  [0.28876, -0.55541, 0.083178, -0.19359, 0.3757...\n",
              "3        Japan         3  [-0.44528, -0.17553, 0.075346, 0.0048481, 0.23..."
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>entity</th>\n      <th>group_id</th>\n      <th>vector</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>London</td>\n      <td>0</td>\n      <td>[-0.040734004, 0.24897666, 0.082936674, -0.140...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>Rome, Italy</td>\n      <td>1</td>\n      <td>[0.041836005, 0.30547366, -0.11563143, -0.2124...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>Tokyo</td>\n      <td>2</td>\n      <td>[0.28876, -0.55541, 0.083178, -0.19359, 0.3757...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>Japan</td>\n      <td>3</td>\n      <td>[-0.44528, -0.17553, 0.075346, 0.0048481, 0.23...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 268
        }
      ],
      "source": [
        "# sample location initial value\n",
        "location_entity_table = pd.DataFrame({'entity':'','group_id':int()},index=[])\n",
        "\n",
        "location_entity_table=location_entity_table.append({'entity':'London','group_id':0,'vector':nlp('London, UK').vector},ignore_index=True)\n",
        "location_entity_table=location_entity_table.append({'entity':'Rome, Italy','group_id':1,'vector':nlp('Rome, Italy').vector},ignore_index=True)\n",
        "location_entity_table=location_entity_table.append({'entity':'Tokyo','group_id':2,'vector':nlp('Tokyo').vector},ignore_index=True)\n",
        "location_entity_table=location_entity_table.append({'entity':'Japan','group_id':3,'vector':nlp('Japan').vector},ignore_index=True)\n",
        "\n",
        "location_entity_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCwJLWbeAE7S"
      },
      "source": [
        "# company_grouper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_urls(tag, n, language):\n",
        "    urls = [url for url in search(tag, stop=n, lang=language)]\n",
        "    return urls\n",
        "\n",
        "#get_urls('glassmkaing companies uk list',10,'en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_aliases(input_company):\n",
        "    #input_company='m&s'\n",
        "    alias_list=[]\n",
        "    try:\n",
        "        link=get_urls(input_company+' company wikipedia page',1,'en')[0]\n",
        "        print('link from google: ',link)\n",
        "        if 'wikipedia' in link:\n",
        "            link=re.sub('.+wiki\\/','',link)\n",
        "        print('cleaned name from link: ',link)\n",
        "        input_company=link\n",
        "        site = pywikibot.Site(\"en\", \"wikipedia\")\n",
        "        page = pywikibot.Page(site,input_company)\n",
        "        item = pywikibot.ItemPage.fromPage(page)\n",
        "        item_dict = item.get()\n",
        "        alias_list=item_dict['aliases']['en']\n",
        "    except Exception as e:\n",
        "        print('error in google serp api: ',e)\n",
        "        alias_list=[]\n",
        "\n",
        "\n",
        "    return alias_list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfjBheVsAIUz"
      },
      "source": [
        "def add_company_aliases_to_table(input_company,group_id):\n",
        "    input_company=input_company.lower()\n",
        "    global company_name_entity_table\n",
        "    alias_list=[]\n",
        "    flag=True\n",
        "\n",
        "    try:\n",
        "\n",
        "        alias_list=find_aliases(input_company)\n",
        "\n",
        "        #check similarity of the input entit yand the aliases found for that entity to make sure we have the right page\n",
        "        for i in alias_list:\n",
        "\n",
        "            #print(input_company, i)\n",
        "\n",
        "            if fuzz.partial_ratio(input_company, i.lower())>=90:\n",
        "                flag=True\n",
        "                print('wiki page found is relevant')\n",
        "                break\n",
        "            else:\n",
        "                flag=False\n",
        "\n",
        "        if flag==False:\n",
        "            raise Exception('wiki page aliases didnt match so adding the enitity as new entry in table')\n",
        "\n",
        "        if input_company not in alias_list:\n",
        "            alias_list.append(input_company)\n",
        "            \n",
        "        for i in alias_list:\n",
        "            company_name_entity_table=company_name_entity_table.append({'entity':i.lower(),'group_id':group_id},ignore_index=True)\n",
        "\n",
        "    except Exception as e:\n",
        "            print('error occurred: ',e)\n",
        "            print('adding the entity as new entry')\n",
        "            company_name_entity_table=company_name_entity_table.append({'entity':input_company,'group_id':group_id},ignore_index=True)\n",
        "\n",
        "    return"
      ],
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {},
      "outputs": [],
      "source": [
        "company_name_entity_table = pd.DataFrame({'entity':'','group_id':int()},index=[])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbS4M4aSAenc",
        "tags": []
      },
      "source": [
        "# add values in the sample database\n",
        "add_company_aliases_to_table(input_company='walmart',group_id=0)\n",
        "add_company_aliases_to_table(input_company='tesco',group_id=1)\n",
        "add_company_aliases_to_table(input_company='google',group_id=2)"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "link from google:  https://en.wikipedia.org/wiki/Walmart\n",
            "cleaned name from link:  Walmart\n",
            "wiki page found is relevant\n",
            "link from google:  https://en.wikipedia.org/wiki/Tesco\n",
            "cleaned name from link:  Tesco\n",
            "wiki page found is relevant\n",
            "link from google:  https://en.wikipedia.org/wiki/Google\n",
            "cleaned name from link:  Google\n",
            "wiki page found is relevant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQJhM2gHAiUm"
      },
      "source": [
        "company_name_entity_table"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   entity  group_id\n",
              "0                wal-mart         0\n",
              "1                wal mart         0\n",
              "2   wal-mart stores, inc.         0\n",
              "3           walmart, inc.         0\n",
              "4            walmart inc.         0\n",
              "5                wallmart         0\n",
              "6                wal-mart         0\n",
              "7               wall mart         0\n",
              "8               wall-mart         0\n",
              "9                 walmart         0\n",
              "10              tesco plc         1\n",
              "11                  tesco         1\n",
              "12            google inc.         2\n",
              "13             google llc         2\n",
              "14                 google         2"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>entity</th>\n      <th>group_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>wal-mart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>wal mart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>wal-mart stores, inc.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>walmart, inc.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>walmart inc.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>wallmart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>wal-mart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>wall mart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>wall-mart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>walmart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>tesco plc</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>tesco</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>google inc.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>google llc</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>google</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ38nWBNAjMl"
      },
      "source": [
        "def company_grouper(input_company):\n",
        "    input_company=input_company.lower()\n",
        "    global company_name_entity_table\n",
        "    flag=False\n",
        "    for index, row in company_name_entity_table.iterrows():\n",
        "        if fuzz.partial_ratio(input_company, row['entity'])>=90:\n",
        "            #print(input_company, row['entity'])\n",
        "            flag=True\n",
        "            group_id=row['group_id']\n",
        "            company_name_entity_table=company_name_entity_table.append({'entity':input_company,'group_id':group_id},ignore_index=True)\n",
        "            print('entity grouped: ',{'entity':input_company,'group_id':group_id})\n",
        "            break\n",
        "        else:\n",
        "            flag=False\n",
        "            pass\n",
        "\n",
        "    if flag==False:\n",
        "        print('entity not present in the existing database, trying to find aliases on wikidata before adding it')\n",
        "        last=max(list(company_name_entity_table['group_id']))\n",
        "        add_company_aliases_to_table(input_company=input_company,group_id=last+1)\n",
        "\n",
        "    return \n",
        "\n"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "source": [
        "# address_grouper"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "wTuv_K59Am8f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# This class uses model intelligent-address-parser-model-40mn.h5\n",
        "class AddressParser():\n",
        "    # ## define the vocab\n",
        "    vocab = list(string.whitespace + string.digits +\n",
        "                 string.ascii_lowercase + string.punctuation)  # total 74 characters\n",
        "    # ## create the Tokenizer object, word_index contaning all the chars in the vocab\n",
        "    tokenizer = Tokenizer(num_words=len(vocab), filters='',\n",
        "                          char_level=True, oov_token='<oov>')\n",
        "    tokenizer.fit_on_texts(vocab)\n",
        "\n",
        "    labels_dict = {  # we are keeping the value part as strings as it easy to generate labels , we could just give len(string)*label value to generate the label\n",
        "        'PADDING': 0,\n",
        "        'SEPARATOR':  1,\n",
        "        'SCHAR': 2,\n",
        "\n",
        "        'SUB_ORGANISATION': 3,\n",
        "        'ORGANISATION': 4,\n",
        "        'SUB_BUILDING_NAME': 5,\n",
        "        'SUB_BUILDING_NUMBER': 6,\n",
        "        'BUILDING_NAME': 7,\n",
        "        'BUILDING_NUMBER': 8,\n",
        "        'PO_BOX_NUMBER': 9,\n",
        "        'SUB_STREET': 10,\n",
        "        'STREET': 11,\n",
        "        'SUB_LOCALITY': 12,\n",
        "        'LOCALITY': 13,\n",
        "        'TOWN': 14,\n",
        "        'POSTCODE': 15\n",
        "    }\n",
        "\n",
        "    popper = ['SEPARATOR', 'PADDING', 'SCHAR']\n",
        "    reversed_labels = {value: key for (key, value) in labels_dict.items()}\n",
        "    model = keras.models.load_model('intelligent-address-parser-model-40mn.h5') # load pretrained model\n",
        "    maxlen = 100\n",
        "\n",
        "    def get_parsed_address(self, address_text=None):\n",
        "        dicta = {}\n",
        "        token_list = self.tokenizer.texts_to_sequences([address_text])[0]\n",
        "        token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], padding='post', value=0, dtype='int',\n",
        "                                                                   maxlen=self.maxlen, truncating='pre')\n",
        "        # notice that we are convertint the values in token_list as list of lists , that is what the pad_sequences takes as input\n",
        "        # call the prediction function\n",
        "        predicted = self.model.predict_classes(token_list, verbose=0)\n",
        "        # doing this so that we can iterate over the elements, and also helps in discarding the padding part\n",
        "        list_p = [i for i in zip(address_text, predicted[0])]\n",
        "        for x in range(len(list_p)):\n",
        "            i = list_p[x]\n",
        "            label_found = self.reversed_labels[i[1]]\n",
        "            if label_found not in dicta:\n",
        "                dicta[label_found] = i[0]\n",
        "            else:\n",
        "                dicta[label_found] += i[0]\n",
        "                if (x + 1) != len(list_p) and list_p[x + 1][1] != list_p[x][1]:\n",
        "                    # adding a space if the next element in the list_p belongs to another label, this maintins the spaces between words\n",
        "                    dicta[label_found] += ' '\n",
        "\n",
        "        for item in dicta:\n",
        "            a = dicta[item]\n",
        "            dicta[item] = a.strip()\n",
        "\n",
        "        dicta.pop('SEPARATOR', None)\n",
        "        dicta.pop('PADDING', None)\n",
        "        dicta.pop('SCHAR', None)\n",
        "        dicta = {key: val for key, val in dicta.items(\n",
        "        ) if val not in ('', ' ', ',', '-', '.', \"'\")}\n",
        "        return dicta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BUILDING_NUMBER': '152',\n",
              " 'STREET': 'longcroft lane',\n",
              " 'TOWN': 'welwyn garden city',\n",
              " 'POSTCODE': 'al8 6en'}"
            ]
          },
          "metadata": {},
          "execution_count": 250
        }
      ],
      "source": [
        "# trying the address parser\n",
        "address=AddressParser()\n",
        "address.get_parsed_address(address_text='152, longcroft lane, welwyn garden city, al8 6en')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create initial table\n",
        "columns={\n",
        "        'SUB_ORGANISATION': '',\n",
        "        'ORGANISATION': '',\n",
        "        'SUB_BUILDING_NAME': '',\n",
        "        'SUB_BUILDING_NUMBER': '',\n",
        "        'BUILDING_NAME': '',\n",
        "        'BUILDING_NUMBER': '',\n",
        "        'PO_BOX_NUMBER': '',\n",
        "        'SUB_STREET': '',\n",
        "        'STREET': '',\n",
        "        'SUB_LOCALITY': '',\n",
        "        'LOCALITY': '',\n",
        "        'TOWN': '',\n",
        "        'POSTCODE': ''}\n",
        "address_entity_table = pd.DataFrame(columns,index=[])\n",
        "address_entity_table['group_id']=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [SUB_ORGANISATION, ORGANISATION, SUB_BUILDING_NAME, SUB_BUILDING_NUMBER, BUILDING_NAME, BUILDING_NUMBER, PO_BOX_NUMBER, SUB_STREET, STREET, SUB_LOCALITY, LOCALITY, TOWN, POSTCODE, group_id]\n",
              "Index: []"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SUB_ORGANISATION</th>\n      <th>ORGANISATION</th>\n      <th>SUB_BUILDING_NAME</th>\n      <th>SUB_BUILDING_NUMBER</th>\n      <th>BUILDING_NAME</th>\n      <th>BUILDING_NUMBER</th>\n      <th>PO_BOX_NUMBER</th>\n      <th>SUB_STREET</th>\n      <th>STREET</th>\n      <th>SUB_LOCALITY</th>\n      <th>LOCALITY</th>\n      <th>TOWN</th>\n      <th>POSTCODE</th>\n      <th>group_id</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 252
        }
      ],
      "source": [
        "address_entity_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add addresses to the database after parsing them\n",
        "def add_address_to_table(address_text,group_id,columns=columns):\n",
        "    global address_entity_table\n",
        "    address=AddressParser()\n",
        "    parsed_dict=address.get_parsed_address(address_text=address_text)\n",
        "    row={**columns, **parsed_dict}\n",
        "    row['group_id']=group_id\n",
        "    address_entity_table=address_entity_table.append(row, ignore_index=True)\n",
        "    print('address saved in the database')\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "address saved in the database\naddress saved in the database\naddress saved in the database\n"
          ]
        }
      ],
      "source": [
        "add_address_to_table(address_text='395 king street, aberdeen, ab24 5rp',group_id=0)\n",
        "add_address_to_table(address_text='152, longcroft lane, welwyen garden city, al8 6en',group_id=1)\n",
        "add_address_to_table(address_text='30 gresham street, london, england, ec2v 7qp',group_id=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  SUB_ORGANISATION ORGANISATION SUB_BUILDING_NAME SUB_BUILDING_NUMBER  \\\n",
              "0                                                                       \n",
              "1                                                                       \n",
              "2                                                                       \n",
              "\n",
              "  BUILDING_NAME BUILDING_NUMBER PO_BOX_NUMBER SUB_STREET          STREET  \\\n",
              "0                           395                              king street   \n",
              "1                           152                           longcroft lane   \n",
              "2                            30                           gresham street   \n",
              "\n",
              "  SUB_LOCALITY LOCALITY                 TOWN  POSTCODE  group_id  \n",
              "0                                   aberdeen  ab24 5rp         0  \n",
              "1                        welwyen garden city   al8 6en         1  \n",
              "2               england               london  ec2v 7qp         2  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SUB_ORGANISATION</th>\n      <th>ORGANISATION</th>\n      <th>SUB_BUILDING_NAME</th>\n      <th>SUB_BUILDING_NUMBER</th>\n      <th>BUILDING_NAME</th>\n      <th>BUILDING_NUMBER</th>\n      <th>PO_BOX_NUMBER</th>\n      <th>SUB_STREET</th>\n      <th>STREET</th>\n      <th>SUB_LOCALITY</th>\n      <th>LOCALITY</th>\n      <th>TOWN</th>\n      <th>POSTCODE</th>\n      <th>group_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>395</td>\n      <td></td>\n      <td></td>\n      <td>king street</td>\n      <td></td>\n      <td></td>\n      <td>aberdeen</td>\n      <td>ab24 5rp</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>152</td>\n      <td></td>\n      <td></td>\n      <td>longcroft lane</td>\n      <td></td>\n      <td></td>\n      <td>welwyen garden city</td>\n      <td>al8 6en</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>30</td>\n      <td></td>\n      <td></td>\n      <td>gresham street</td>\n      <td></td>\n      <td>england</td>\n      <td>london</td>\n      <td>ec2v 7qp</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 255
        }
      ],
      "source": [
        "# initial values\n",
        "address_entity_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fuzzy_match_address(parsed_address,columns=columns):\n",
        "    global address_entity_table\n",
        "    group_id=int()\n",
        "    score_collector=[]\n",
        "    for index, row in address_entity_table.iterrows():\n",
        "        row_score=0\n",
        "        for key in parsed_address.keys():\n",
        "            row_score=row_score+fuzz.partial_ratio(row[key],parsed_address[key])\n",
        "        score_collector.append(row_score/len(parsed_address)) # average the score, so max value will be 100\n",
        "        print('simialrity score: ',row_score/len(parsed_address))\n",
        "    \n",
        "    max_val=max(score_collector)\n",
        "    row={**columns, **parsed_address}\n",
        "\n",
        "    if max_val>=90: # grouped\n",
        "        group_id=address_entity_table['group_id'][score_collector.index(max_val)]\n",
        "        print('address grouped')\n",
        "        row['group_id']=group_id\n",
        "\n",
        "    else: # add as new entity assign new group_id\n",
        "        last=max(list(address_entity_table['group_id']))\n",
        "        print('new address discovered')\n",
        "        row['group_id']=last+1\n",
        "\n",
        "    address_entity_table=address_entity_table.append(row, ignore_index=True)\n",
        "    print('address saved in the database')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {},
      "outputs": [],
      "source": [
        "def address_grouper(address_text):\n",
        "    global address_entity_table\n",
        "    address=AddressParser()\n",
        "    parsed_address=address.get_parsed_address(address_text=address_text)\n",
        "    fuzzy_match_address(parsed_address)\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY9XNU9h0sNL"
      },
      "source": [
        "# classifier_main_function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LToRo6WV0g8Y"
      },
      "source": [
        "# this is the master function which will be calling all other functions\n",
        "def classifier_main_function(input_entity):\n",
        "    global model\n",
        "    global tokenizer\n",
        "    global serial_number_entity_table\n",
        "    global physical_good_entity_table\n",
        "    global location_entity_table\n",
        "    # prediction (this is the starting point of our full model)\n",
        "\n",
        "    sentence = [input_entity] # convert into list\n",
        "    sequences = tokenizer.texts_to_sequences(sentence)\n",
        "    padded = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='pre')\n",
        "    output_class=reversed_labels[np.argmax(model.predict(padded)[0], axis=-1)]\n",
        "    print('the input entity is classified as: ', output_class)\n",
        "\n",
        "\n",
        "    if output_class=='serial_number':\n",
        "        serial_grouper(input_entity)\n",
        "        print('-'*60)\n",
        "        print(serial_number_entity_table)\n",
        "\n",
        "    elif output_class=='physical_good':\n",
        "        goods_grouper(input_entity)\n",
        "        print('-'*60)\n",
        "        print(physical_good_entity_table)\n",
        "\n",
        "    elif output_class=='location':\n",
        "        location_grouper(input_entity)\n",
        "        print('-'*60)\n",
        "        print(location_entity_table)\n",
        "        \n",
        "    elif output_class=='company_name':\n",
        "        company_grouper(input_entity)\n",
        "        print('-'*60)\n",
        "        print(company_name_entity_table)\n",
        "\n",
        "    elif output_class=='company_address':\n",
        "        address_grouper(input_entity)\n",
        "        print('-'*60)\n",
        "        print(address_entity_table)\n",
        "\n",
        "    return\n",
        "    "
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08A2Z8lQ2VDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf1d8ac-2fae-4b8a-d034-8d62cb7366a0"
      },
      "source": [
        "# calling the master function\n",
        "input_entity=input('enter entity')\n",
        "classifier_main_function(input_entity)"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the input entity is classified as:  location\n1.0\n0.6525508165359497\n0.45524969696998596\n0.5104739665985107\ncandidates: [0]\ninput_entity_modified ['London', ' UK']\ncan_string:  ['London']\nin_vec_list length 2\ncan_vec_list length 1\nexecuting : in2 and can1\ncandidate_score_list:  [1.0]\nmax sim: 1.0\nentity grouped:  {'entity': 'London, UK', 'group_id': 0}\n------------------------------------------------------------\n        entity  group_id                                             vector\n0       London         0  [-0.040734004, 0.24897666, 0.082936674, -0.140...\n1  Rome, Italy         1  [0.041836005, 0.30547366, -0.11563143, -0.2124...\n2        Tokyo         2  [0.28876, -0.55541, 0.083178, -0.19359, 0.3757...\n3        Japan         3  [-0.44528, -0.17553, 0.075346, 0.0048481, 0.23...\n4   London, UK         0  [-0.040734004, 0.24897666, 0.082936674, -0.140...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}