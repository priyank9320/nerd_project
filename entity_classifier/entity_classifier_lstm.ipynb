{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "entity_classifier_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.4-final"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo7CtpewFt51"
      },
      "source": [
        "Description: \n",
        "First we train a Bi-LSTM character level classifier which is used to classify the entire input string into one of 5 categories. After classification, we try to group the entity with the already present entities according to simialrity criterias. If the entity is not similar enough to the already present entities it will be recorded as a new entity and assigned a new group id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3ktt5nmDCEq"
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install pywikibot\n",
        "!pip install jellyfish\n",
        "!pip install rapidfuzz\n",
        "!pip install autocorrect\n",
        "!pip install spacy==2.2.4\n"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2PakEB-nUln"
      },
      "source": [
        "import pywikibot\n",
        "from rapidfuzz import fuzz \n",
        "from pprint import pprint\n",
        "import en_core_web_lg\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "\n",
        "from scipy import spatial\n",
        "from autocorrect import Speller\n",
        "\n",
        "import re\n",
        "\n",
        "from googlesearch import search\n"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSY7fMTgDwTp"
      },
      "source": [
        "import string\n",
        "from random import randrange\n",
        "import random  # used to create random separator\n",
        "import string  # this is used to create the vocab of all the characters , stored in the variable 'vocab'\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "import csv\n",
        "import pickle\n",
        "\n",
        "# below packages are used to create the model\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Bidirectional, TimeDistributed, Input, Masking\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n070HCcGQBZe"
      },
      "source": [
        "# max length of the input\n",
        "maxlen=100"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9v4nP5R5mPa"
      },
      "source": [
        "# load the csv data file\n",
        "df=pd.read_csv('datafile_lstm.csv')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1c5g1hyRg6V"
      },
      "source": [
        "# shuffle data frame\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl0w9Aj12BYB"
      },
      "source": [
        "# split the dataframe into train and test\n",
        "df1=df.iloc[100:500,:]\n",
        "df2=df.iloc[:100,:]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv2_qn6e13xF"
      },
      "source": [
        "# Dictionary contains the Labels\n",
        "labels_dict = {  # we are keeping the value part as strings as it easy to generate labels , we could just give len(string)*label value to generate the label\n",
        "    'company_name': 0,\n",
        "    'company_address': 1,\n",
        "    'serial_number': 2,\n",
        "    'physical_good': 3,\n",
        "    'location': 4\n",
        "}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBGS7WJP18UR",
        "outputId": "2b0e5d7e-398a-4efa-ec2d-7970b3856065"
      },
      "source": [
        "# create reverse dictionary of label placeholders\n",
        "reversed_labels = {value: key for (key, value) in labels_dict.items()}\n",
        "reversed_labels"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'company_name',\n",
              " 1: 'company_address',\n",
              " 2: 'serial_number',\n",
              " 3: 'physical_good',\n",
              " 4: 'location'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2LvIHDsD50q"
      },
      "source": [
        "# define the vocab\n",
        "vocab = list(string.whitespace + string.digits +\n",
        "             string.ascii_lowercase + string.punctuation)  # total 74 characters"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxqdzJTkPzZH",
        "outputId": "8a304f34-7795-4bac-c8d1-4426f038fb24"
      },
      "source": [
        "vocab_size=len(vocab)\n",
        "vocab_size"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTje8-NnEJE1"
      },
      "source": [
        "# create the Tokenizer object, word_index contaning all the chars in the vocab\n",
        "# basically this will encode all the characters into numbers, so each character defined in our vocab will be represented by a number\n",
        "# vocab is just a list of tokens\n",
        "tokenizer = Tokenizer(num_words=len(vocab), filters='',\n",
        "                      char_level=True, oov_token='<oov>')\n",
        "tokenizer.fit_on_texts(vocab)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSHrIukISxdE"
      },
      "source": [
        "#train\n",
        "train_sentences=list(df1['text'])\n",
        "train_labels=list(df1['category'])\n",
        "train_sequences=tokenizer.texts_to_sequences(train_sentences)\n",
        "train_padded=pad_sequences(train_sequences, padding='post',maxlen=maxlen, truncating='pre') #maxlen decides the maximum length of the sequence\n",
        "train_labels=[labels_dict[item] for item in train_labels ]\n",
        "train_labels = np.asarray(train_labels, dtype=np.int32)\n",
        "train_label_one_hot = tf.keras.utils.to_categorical(train_labels, dtype='int32')\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRmyMQFhR6Hz"
      },
      "source": [
        "#test\n",
        "test_sentences=list(df2['text'])\n",
        "test_labels=list(df2['category'])\n",
        "test_sequences=tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded=pad_sequences(test_sequences, padding='post',maxlen=maxlen, truncating='pre') #maxlen decides the maximum length of the sequence\n",
        "test_labels=[labels_dict[item] for item in test_labels ]\n",
        "test_labels = np.asarray(test_labels, dtype=np.int32)\n",
        "test_label_one_hot = tf.keras.utils.to_categorical(test_labels, dtype='int32')\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcBFNuK1FKJm",
        "outputId": "6036396e-dd05-4927-f0df-fd33c4fd21d5"
      },
      "source": [
        "# Create model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size+1, output_dim=32,\n",
        "                    input_length=maxlen, mask_zero=True))\n",
        "# vocab_size+1 is to inlcude the padding\n",
        "# output_dim is the size of the embedding for the characters in the vocab\n",
        "model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
        "#model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
        "# 5 is the number of labels and which we need in the softamx classifier\n",
        "model.add(Dense(len(labels_dict), activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 100, 32)           2400      \n_________________________________________________________________\nbidirectional (Bidirectional (None, 64)                16640     \n_________________________________________________________________\ndense (Dense)                (None, 5)                 325       \n=================================================================\nTotal params: 19,365\nTrainable params: 19,365\nNon-trainable params: 0\n_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCOwOI99FSVq"
      },
      "source": [
        "# create checkpoint\n",
        "checkpoint_filepath = 'model_checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    save_best_only=False,\n",
        "    monitor='val_accuracy',\n",
        "    save_freq='epoch',\n",
        "    mode='max',\n",
        "    verbose=0,\n",
        "    options=None\n",
        ")\n",
        "adam = keras.optimizers.Adam(learning_rate=0.001015)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_padded, train_label_one_hot,batch_size=32, epochs=10,  # TRAIN THE MODEL\n",
        "                    validation_data=(test_padded, test_label_one_hot)#, callbacks=[model_checkpoint_callback]\n",
        "                    )\n",
        "model.save('model/intelligent-address-parser-model.h5')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "13/13 [==============================] - 15s 394ms/step - loss: 1.6018 - accuracy: 0.2834 - val_loss: 1.5797 - val_accuracy: 0.3600\n",
            "Epoch 2/10\n",
            "13/13 [==============================] - 1s 110ms/step - loss: 1.5592 - accuracy: 0.4672 - val_loss: 1.4895 - val_accuracy: 0.5400\n",
            "Epoch 3/10\n",
            "13/13 [==============================] - 1s 109ms/step - loss: 1.4051 - accuracy: 0.5779 - val_loss: 1.1979 - val_accuracy: 0.3700\n",
            "Epoch 4/10\n",
            "13/13 [==============================] - 1s 110ms/step - loss: 1.0720 - accuracy: 0.5156 - val_loss: 0.9908 - val_accuracy: 0.7100\n",
            "Epoch 5/10\n",
            "13/13 [==============================] - 1s 96ms/step - loss: 0.8396 - accuracy: 0.7350 - val_loss: 0.8083 - val_accuracy: 0.8100\n",
            "Epoch 6/10\n",
            "13/13 [==============================] - 1s 91ms/step - loss: 0.7092 - accuracy: 0.7737 - val_loss: 0.6859 - val_accuracy: 0.8100\n",
            "Epoch 7/10\n",
            "13/13 [==============================] - 1s 104ms/step - loss: 0.6035 - accuracy: 0.7805 - val_loss: 0.5946 - val_accuracy: 0.8600\n",
            "Epoch 8/10\n",
            "13/13 [==============================] - 1s 99ms/step - loss: 0.5046 - accuracy: 0.8292 - val_loss: 0.6112 - val_accuracy: 0.7600\n",
            "Epoch 9/10\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 0.4549 - accuracy: 0.8147 - val_loss: 0.4888 - val_accuracy: 0.8100\n",
            "Epoch 10/10\n",
            "13/13 [==============================] - 2s 190ms/step - loss: 0.3918 - accuracy: 0.8478 - val_loss: 0.4360 - val_accuracy: 0.8500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWVbup2dDG_A"
      },
      "source": [
        "# Entity Disambiguation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eElXBELguc3c"
      },
      "source": [
        "# load the english spacy model\n",
        "# this is used to get vectors for text\n",
        "nlp = en_core_web_lg.load()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YnKifKQjCeY"
      },
      "source": [
        "# serial_grouper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XVmZlpg-sQH"
      },
      "source": [
        "#decide to group or to add as new entity\n",
        "#from rapidfuzz import fuzz\n",
        "\n",
        "def serial_grouper(input_string):\n",
        "    global serial_number_entity_table\n",
        "    #input_string=sentence\n",
        "    grouped_flag=0\n",
        "    for index, row in serial_number_entity_table.iterrows():\n",
        "\n",
        "        if re.findall(\"\\A\"+input_string[0:3], row['entity']): # check if the starting 3 characters are same, if yes then group them\n",
        "            # save the entity in table and assign the same group id\n",
        "            serial_number_entity_table=serial_number_entity_table.append({'entity':input_string, 'group_id':row['group_id']},ignore_index=True)\n",
        "            print('entity grouped: ',{'entity':input_string, 'group_id':row['group_id']})\n",
        "            grouped_flag=1\n",
        "            break\n",
        "        \n",
        "\n",
        "    if grouped_flag==0:\n",
        "        # save it as a new entity\n",
        "        last=max(list(serial_number_entity_table['group_id']))\n",
        "        serial_number_entity_table=serial_number_entity_table.append({'entity':input_string,'group_id':last+1},ignore_index=True)\n",
        "        print('entity saved as new entry in the table: ', {'entity':input_string,'group_id':last+1})  \n",
        "        \n",
        "    return"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           entity  group_id\n",
              "0        abc12345         0\n",
              "1    bhkdj9849204         1\n",
              "2  zlljdoi9720483         2\n",
              "3    bbnvnv909090         3"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>entity</th>\n      <th>group_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>abc12345</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>bhkdj9849204</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>zlljdoi9720483</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>bbnvnv909090</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 210
        }
      ],
      "source": [
        "# sample serial numbers inital values\n",
        "serial_number_entity_table = pd.DataFrame({'entity':'','group_id':int()},index=[])\n",
        "\n",
        "serial_number_entity_table=pd.DataFrame({'entity':['abc12345','bhkdj9849204','zlljdoi9720483','bbnvnv909090'],\n",
        "                                   'group_id':[0,1,2,3]})\n",
        "serial_number_entity_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tEd9z1Yi6xj"
      },
      "source": [
        "# goods_grouper\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPNUp8hHxx1i"
      },
      "source": [
        "# importing spelling corrector, we can easily use spelling correction as physical goods are very common and new names dont come in this\n",
        "spell = Speller(lang='en')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrL8KHWGl7Qc"
      },
      "source": [
        "def goods_grouper(input_entity):\n",
        "    global physical_good_entity_table\n",
        "    #input_entity='chair'\n",
        "    input_entity=spell(input_entity) # spelling correction # physical goods have very general name and so we can easily apply spelling correction\n",
        "\n",
        "    input_vector=nlp(input_entity).vector\n",
        "    list_of_similarities=[]\n",
        "    for index, row in physical_good_entity_table.iterrows():\n",
        "        result = 1 - spatial.distance.cosine(row['vector'], input_vector )\n",
        "        print(result)\n",
        "        list_of_similarities.append(result)\n",
        "\n",
        "    max_sim=max(list_of_similarities)\n",
        "    print('max sim:',max_sim)\n",
        "    if max_sim>=0.72:\n",
        "        group_id=physical_good_entity_table['group_id'][list_of_similarities.index(max_sim)]\n",
        "        physical_good_entity_table=physical_good_entity_table.append({'entity':input_entity,'group_id':group_id,'vector':input_vector},ignore_index=True)\n",
        "        print('entity grouped: ',{'entity':input_entity,'group_id':group_id})\n",
        "\n",
        "    else:\n",
        "        last=max(list(physical_good_entity_table['group_id']))\n",
        "        print('last',last)\n",
        "        physical_good_entity_table=physical_good_entity_table.append({'entity':input_entity,'group_id':last+1,'vector':input_vector},ignore_index=True)\n",
        "        print('new entity added: ',{'entity':input_entity,'group_id':last+1})\n",
        "\n",
        "\n",
        "    return"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           entity  group_id                                             vector\n",
              "0  plastic bottle         0  [-0.289565, -0.049595, -0.077429, -0.15209301,...\n",
              "1      steel bowl         1  [0.12778, 0.25831202, 0.45532, -0.372285, -0.1...\n",
              "2    leather sofa         2  [-0.06037, -0.40972, -0.208975, -0.005081499, ...\n",
              "3  hardwood table         3  [-0.091709, 0.09808999, -0.29916, -0.36364597,..."
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>entity</th>\n      <th>group_id</th>\n      <th>vector</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>plastic bottle</td>\n      <td>0</td>\n      <td>[-0.289565, -0.049595, -0.077429, -0.15209301,...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>steel bowl</td>\n      <td>1</td>\n      <td>[0.12778, 0.25831202, 0.45532, -0.372285, -0.1...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>leather sofa</td>\n      <td>2</td>\n      <td>[-0.06037, -0.40972, -0.208975, -0.005081499, ...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>hardwood table</td>\n      <td>3</td>\n      <td>[-0.091709, 0.09808999, -0.29916, -0.36364597,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 209
        }
      ],
      "source": [
        "# sample physical goods initial values\n",
        "physical_good_entity_table = pd.DataFrame({'entity':'','group_id':int()},index=[])\n",
        "\n",
        "physical_good_entity_table=physical_good_entity_table.append({'entity':'plastic bottle','group_id':0,'vector':nlp('plastic bottle').vector},ignore_index=True)\n",
        "physical_good_entity_table=physical_good_entity_table.append({'entity':'steel bowl','group_id':1,'vector':nlp('steel bowl').vector},ignore_index=True)\n",
        "physical_good_entity_table=physical_good_entity_table.append({'entity':'leather sofa','group_id':2,'vector':nlp('leather sofa').vector},ignore_index=True)\n",
        "physical_good_entity_table=physical_good_entity_table.append({'entity':'hardwood table','group_id':3,'vector':nlp('hardwood table').vector},ignore_index=True)\n",
        "\n",
        "physical_good_entity_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHSjO6aoB-UP"
      },
      "source": [
        "# location_grouper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO2GazHZPddU"
      },
      "source": [
        "vectors are helpful in reducing the search space, or you can say it helps to get candidates then we do string comparisions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdQBGVm2Ck7w"
      },
      "source": [
        "#spell = Speller(lang='en') # spelling correction was not working properly over here\n",
        "\n",
        "def location_grouper(input_entity):\n",
        "    global location_entity_table\n",
        "    #input_entity='London, UK'\n",
        "    #input_entity=spell(input_entity) # spelling correction # physical goods have very general name and so we can easily apply spelling correction\n",
        "\n",
        "    input_vector=nlp(input_entity).vector\n",
        "    list_of_similarities=[]\n",
        "    candidates=[]\n",
        "    for index, row in location_entity_table.iterrows():\n",
        "        result = 1 - spatial.distance.cosine(row['vector'], input_vector )\n",
        "        print(result)\n",
        "        if result>0.70:\n",
        "            candidates.append(index)\n",
        "        #list_of_similarities.append(result)\n",
        "\n",
        "    print('candidates:',candidates)\n",
        "\n",
        "    if candidates:\n",
        "        # prep input_entity for comparision by splitting it at spaces and\n",
        "        input_entity_modified = input_entity.split(',')\n",
        "        print('input_entity_modified',input_entity_modified)\n",
        "        in_vec_list=[] # list of individual vectors for the entity parts\n",
        "        for item in input_entity_modified:\n",
        "            in_vec_list.append(nlp(item.strip()).vector)\n",
        "            \n",
        "\n",
        "        # for each candidate , replace comma with space and split at space, and then compare the vectors\n",
        "\n",
        "        candidate_score_list=[]\n",
        "        for item in candidates :\n",
        "            can_string=location_entity_table['entity'][item] # replace comma with space\n",
        "            can_string=can_string.split(',')\n",
        "\n",
        "            print('can_string: ',can_string)\n",
        "            can_vec_list=[] # list of individual vectors for the entity parts\n",
        "            for i in can_string:\n",
        "                can_vec_list.append(nlp(i.strip()).vector)\n",
        "\n",
        "            # now we compare the elements from the 2 vector lists\n",
        "            print('in_vec_list length',len(in_vec_list))\n",
        "            print('can_vec_list length',len(can_vec_list))\n",
        "\n",
        "            if len(in_vec_list)==1 and len(can_vec_list)==1:\n",
        "                candidate_score_list.append(1 - spatial.distance.cosine(can_vec_list[0], in_vec_list[0]))\n",
        "\n",
        "            elif len(in_vec_list)==1 and len(can_vec_list)==2:# first part is important\n",
        "                candidate_score_list.append(1 - spatial.distance.cosine(can_vec_list[0], in_vec_list[0]))\n",
        "\n",
        "            elif len(in_vec_list)==2 and len(can_vec_list)==1: # first part is important\n",
        "                print('executing : in2 and can1')\n",
        "                candidate_score_list.append(1 - spatial.distance.cosine(can_vec_list[0], in_vec_list[0]))\n",
        "\n",
        "            elif len(in_vec_list)==2 and len(can_vec_list)==2: # least score wil determine \n",
        "                candidate_score_list.append(min([1 - spatial.distance.cosine(can_vec_list[0], in_vec_list[0]), 1 - spatial.distance.cosine(can_vec_list[1], in_vec_list[1])]))\n",
        "\n",
        "            else:\n",
        "                print('no criteria executed')\n",
        "\n",
        "\n",
        "        print('candidate_score_list: ',candidate_score_list)\n",
        "        max_sim=max(candidate_score_list)\n",
        "\n",
        "        print('max sim:',max_sim)\n",
        "        if max_sim>0.85:\n",
        "            group_id=location_entity_table['group_id'][candidates[candidate_score_list.index(max_sim)]]\n",
        "            location_entity_table=location_entity_table.append({'entity':input_entity,'group_id':group_id,'vector':input_vector},ignore_index=True)\n",
        "            print('entity grouped: ',{'entity':input_entity,'group_id':group_id})\n",
        "\n",
        "        else:\n",
        "            last=max(list(location_entity_table['group_id']))\n",
        "            location_entity_table=location_entity_table.append({'entity':input_entity,'group_id':last+1,'vector':input_vector},ignore_index=True)\n",
        "            print('entity added: ',{'entity':input_entity,'group_id':last+1})\n",
        "\n",
        "    else:\n",
        "        last=max(list(location_entity_table['group_id']))\n",
        "        location_entity_table=location_entity_table.append({'entity':input_entity,'group_id':last+1,'vector':input_vector},ignore_index=True)\n",
        "        print('entity added: ',{'entity':input_entity,'group_id':last+1})"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        entity  group_id                                             vector\n",
              "0       London         0  [-0.040734004, 0.24897666, 0.082936674, -0.140...\n",
              "1  Rome, Italy         1  [0.041836005, 0.30547366, -0.11563143, -0.2124...\n",
              "2        Tokyo         2  [0.28876, -0.55541, 0.083178, -0.19359, 0.3757...\n",
              "3        Japan         3  [-0.44528, -0.17553, 0.075346, 0.0048481, 0.23..."
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>entity</th>\n      <th>group_id</th>\n      <th>vector</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>London</td>\n      <td>0</td>\n      <td>[-0.040734004, 0.24897666, 0.082936674, -0.140...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>Rome, Italy</td>\n      <td>1</td>\n      <td>[0.041836005, 0.30547366, -0.11563143, -0.2124...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>Tokyo</td>\n      <td>2</td>\n      <td>[0.28876, -0.55541, 0.083178, -0.19359, 0.3757...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>Japan</td>\n      <td>3</td>\n      <td>[-0.44528, -0.17553, 0.075346, 0.0048481, 0.23...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 170
        }
      ],
      "source": [
        "# sample location initial value\n",
        "location_entity_table = pd.DataFrame({'entity':'','group_id':int()},index=[])\n",
        "\n",
        "location_entity_table=location_entity_table.append({'entity':'London','group_id':0,'vector':nlp('London, UK').vector},ignore_index=True)\n",
        "location_entity_table=location_entity_table.append({'entity':'Rome, Italy','group_id':1,'vector':nlp('Rome, Italy').vector},ignore_index=True)\n",
        "location_entity_table=location_entity_table.append({'entity':'Tokyo','group_id':2,'vector':nlp('Tokyo').vector},ignore_index=True)\n",
        "location_entity_table=location_entity_table.append({'entity':'Japan','group_id':3,'vector':nlp('Japan').vector},ignore_index=True)\n",
        "\n",
        "location_entity_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCwJLWbeAE7S"
      },
      "source": [
        "# company_grouper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_urls(tag, n, language):\n",
        "    urls = [url for url in search(tag, stop=n, lang=language)]\n",
        "    return urls\n",
        "\n",
        "#get_urls('glassmkaing companies uk list',10,'en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_aliases(input_company):\n",
        "    #input_company='m&s'\n",
        "    alias_list=[]\n",
        "    try:\n",
        "        link=get_urls(input_company+' company wikipedia page',1,'en')[0]\n",
        "        print('link from google: ',link)\n",
        "        if 'wikipedia' in link:\n",
        "            link=re.sub('.+wiki\\/','',link)\n",
        "        print('cleaned name from link: ',link)\n",
        "        input_company=link\n",
        "        site = pywikibot.Site(\"en\", \"wikipedia\")\n",
        "        page = pywikibot.Page(site,input_company)\n",
        "        item = pywikibot.ItemPage.fromPage(page)\n",
        "        item_dict = item.get()\n",
        "        alias_list=item_dict['aliases']['en']\n",
        "    except Exception as e:\n",
        "        print('error in google serp api: ',e)\n",
        "        alias_list=[]\n",
        "\n",
        "\n",
        "    return alias_list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfjBheVsAIUz"
      },
      "source": [
        "def add_company_aliases_to_table(input_company,group_id):\n",
        "    input_company=input_company.lower()\n",
        "    global company_name_entity_table\n",
        "    alias_list=[]\n",
        "    flag=True\n",
        "\n",
        "    try:\n",
        "\n",
        "        alias_list=find_aliases(input_company)\n",
        "\n",
        "        #check similarity of the input entit yand the aliases found for that entity to make sure we have the right page\n",
        "        for i in alias_list:\n",
        "\n",
        "            #print(input_company, i)\n",
        "\n",
        "            if fuzz.partial_ratio(input_company, i.lower())>=90:\n",
        "                flag=True\n",
        "                print('wiki page found is relevant')\n",
        "                break\n",
        "            else:\n",
        "                flag=False\n",
        "\n",
        "        if flag==False:\n",
        "            raise Exception('wiki page aliases didnt match so adding the enitity as new entry in table')\n",
        "\n",
        "        if input_company not in alias_list:\n",
        "            alias_list.append(input_company)\n",
        "            \n",
        "        for i in alias_list:\n",
        "            company_name_entity_table=company_name_entity_table.append({'entity':i.lower(),'group_id':group_id},ignore_index=True)\n",
        "\n",
        "    except Exception as e:\n",
        "            print('error occurred: ',e)\n",
        "            print('adding the entity as new entry')\n",
        "            company_name_entity_table=company_name_entity_table.append({'entity':input_company,'group_id':group_id},ignore_index=True)\n",
        "\n",
        "    return"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [],
      "source": [
        "company_name_entity_table = pd.DataFrame({'entity':'','group_id':int()},index=[])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbS4M4aSAenc",
        "tags": []
      },
      "source": [
        "# add values in the sample database\n",
        "add_company_aliases_to_table(input_company='walmart',group_id=0)\n",
        "add_company_aliases_to_table(input_company='tesco',group_id=1)\n",
        "add_company_aliases_to_table(input_company='google',group_id=2)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "link from google:  https://en.wikipedia.org/wiki/Walmart\n",
            "cleaned name from link:  Walmart\n",
            "wiki page found is relevant\n",
            "link from google:  https://en.wikipedia.org/wiki/Tesco\n",
            "cleaned name from link:  Tesco\n",
            "wiki page found is relevant\n",
            "link from google:  https://en.wikipedia.org/wiki/Google\n",
            "cleaned name from link:  Google\n",
            "wiki page found is relevant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQJhM2gHAiUm"
      },
      "source": [
        "company_name_entity_table"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   entity  group_id\n",
              "0                wal-mart         0\n",
              "1                wal mart         0\n",
              "2   wal-mart stores, inc.         0\n",
              "3           walmart, inc.         0\n",
              "4            walmart inc.         0\n",
              "5                wallmart         0\n",
              "6                wal-mart         0\n",
              "7               wall mart         0\n",
              "8               wall-mart         0\n",
              "9                 walmart         0\n",
              "10              tesco plc         1\n",
              "11                  tesco         1\n",
              "12            google inc.         2\n",
              "13             google llc         2\n",
              "14                 google         2"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>entity</th>\n      <th>group_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>wal-mart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>wal mart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>wal-mart stores, inc.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>walmart, inc.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>walmart inc.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>wallmart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>wal-mart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>wall mart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>wall-mart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>walmart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>tesco plc</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>tesco</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>google inc.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>google llc</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>google</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ38nWBNAjMl"
      },
      "source": [
        "def company_grouper(input_company):\n",
        "    input_company=input_company.lower()\n",
        "    global company_name_entity_table\n",
        "    flag=False\n",
        "    for index, row in company_name_entity_table.iterrows():\n",
        "        if fuzz.partial_ratio(input_company, row['entity'])>=90:\n",
        "            #print(input_company, row['entity'])\n",
        "            flag=True\n",
        "            group_id=row['group_id']\n",
        "            company_name_entity_table=company_name_entity_table.append({'entity':input_company,'group_id':group_id},ignore_index=True)\n",
        "            print('entity grouped: ',{'entity':input_company,'group_id':group_id})\n",
        "            break\n",
        "        else:\n",
        "            flag=False\n",
        "            pass\n",
        "\n",
        "    if flag==False:\n",
        "        print('entity not present in the existing database, trying to find aliases on wikidata before adding it')\n",
        "        last=max(list(company_name_entity_table['group_id']))\n",
        "        add_company_aliases_to_table(input_company=input_company,group_id=last+1)\n",
        "\n",
        "    return \n",
        "\n"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "source": [
        "# address_grouper"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "wTuv_K59Am8f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# This class  uses a pre-trained mdoel stored in intelligent_address_parser/model and generates a doctionary of parsed address\n",
        "\n",
        "\n",
        "class AddressParser():\n",
        "    # ## define the vocab\n",
        "    vocab = list(string.whitespace + string.digits +\n",
        "                 string.ascii_lowercase + string.punctuation)  # total 74 characters\n",
        "    # ## create the Tokenizer object, word_index contaning all the chars in the vocab\n",
        "    tokenizer = Tokenizer(num_words=len(vocab), filters='',\n",
        "                          char_level=True, oov_token='<oov>')\n",
        "    tokenizer.fit_on_texts(vocab)\n",
        "\n",
        "    labels_dict = {  # we are keeping the value part as strings as it easy to generate labels , we could just give len(string)*label value to generate the label\n",
        "        'PADDING': 0,\n",
        "        'SEPARATOR':  1,\n",
        "        'SCHAR': 2,\n",
        "\n",
        "        'SUB_ORGANISATION': 3,\n",
        "        'ORGANISATION': 4,\n",
        "        'SUB_BUILDING_NAME': 5,\n",
        "        'SUB_BUILDING_NUMBER': 6,\n",
        "        'BUILDING_NAME': 7,\n",
        "        'BUILDING_NUMBER': 8,\n",
        "        'PO_BOX_NUMBER': 9,\n",
        "        'SUB_STREET': 10,\n",
        "        'STREET': 11,\n",
        "        'SUB_LOCALITY': 12,\n",
        "        'LOCALITY': 13,\n",
        "        'TOWN': 14,\n",
        "        'POSTCODE': 15\n",
        "    }\n",
        "\n",
        "    popper = ['SEPARATOR', 'PADDING', 'SCHAR']\n",
        "    reversed_labels = {value: key for (key, value) in labels_dict.items()}\n",
        "    model = keras.models.load_model('intelligent-address-parser-model-40mn.h5') # load pretrained model\n",
        "    maxlen = 100\n",
        "\n",
        "    def get_parsed_address(self, address_text=None):\n",
        "        dicta = {}\n",
        "        token_list = self.tokenizer.texts_to_sequences([address_text])[0]\n",
        "        token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], padding='post', value=0, dtype='int',\n",
        "                                                                   maxlen=self.maxlen, truncating='pre')\n",
        "        # notice that we are convertint the values in token_list as list of lists , that is what the pad_sequences takes as input\n",
        "        # call the prediction function\n",
        "        predicted = self.model.predict_classes(token_list, verbose=0)\n",
        "        # doing this so that we can iterate over the elements, and also helps in discarding the padding part\n",
        "        list_p = [i for i in zip(address_text, predicted[0])]\n",
        "        for x in range(len(list_p)):\n",
        "            i = list_p[x]\n",
        "            label_found = self.reversed_labels[i[1]]\n",
        "            if label_found not in dicta:\n",
        "                dicta[label_found] = i[0]\n",
        "            else:\n",
        "                dicta[label_found] += i[0]\n",
        "                if (x + 1) != len(list_p) and list_p[x + 1][1] != list_p[x][1]:\n",
        "                    # adding a space if the next element in the list_p belongs to another label, this maintins the spaces between words\n",
        "                    dicta[label_found] += ' '\n",
        "\n",
        "        for item in dicta:\n",
        "            a = dicta[item]\n",
        "            dicta[item] = a.strip()\n",
        "\n",
        "        dicta.pop('SEPARATOR', None)\n",
        "        dicta.pop('PADDING', None)\n",
        "        dicta.pop('SCHAR', None)\n",
        "        dicta = {key: val for key, val in dicta.items(\n",
        "        ) if val not in ('', ' ', ',', '-', '.', \"'\")}\n",
        "        return dicta\n",
        "\n",
        "# This class  performs the address matching by running a sql query in postgres database and returns a pandas dataframe with a list of top 10 matched addresses whose similarity score is more than the threshold specifed in the variable match_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BUILDING_NUMBER': '152',\n",
              " 'STREET': 'longcroft lane',\n",
              " 'TOWN': 'welwyn garden city',\n",
              " 'POSTCODE': 'al8 6en'}"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ],
      "source": [
        "address=AddressParser()\n",
        "address.get_parsed_address(address_text='152, longcroft lane, welwyn garden city, al8 6en')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "# table containing initial values for the addresses\n",
        "columns={\n",
        "        'SUB_ORGANISATION': '',\n",
        "        'ORGANISATION': '',\n",
        "        'SUB_BUILDING_NAME': '',\n",
        "        'SUB_BUILDING_NUMBER': '',\n",
        "        'BUILDING_NAME': '',\n",
        "        'BUILDING_NUMBER': '',\n",
        "        'PO_BOX_NUMBER': '',\n",
        "        'SUB_STREET': '',\n",
        "        'STREET': '',\n",
        "        'SUB_LOCALITY': '',\n",
        "        'LOCALITY': '',\n",
        "        'TOWN': '',\n",
        "        'POSTCODE': ''}\n",
        "address_entity_table = pd.DataFrame(columns,index=[])\n",
        "address_entity_table['group_id']=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [SUB_ORGANISATION, ORGANISATION, SUB_BUILDING_NAME, SUB_BUILDING_NUMBER, BUILDING_NAME, BUILDING_NUMBER, PO_BOX_NUMBER, SUB_STREET, STREET, SUB_LOCALITY, LOCALITY, TOWN, POSTCODE, group_id]\n",
              "Index: []"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SUB_ORGANISATION</th>\n      <th>ORGANISATION</th>\n      <th>SUB_BUILDING_NAME</th>\n      <th>SUB_BUILDING_NUMBER</th>\n      <th>BUILDING_NAME</th>\n      <th>BUILDING_NUMBER</th>\n      <th>PO_BOX_NUMBER</th>\n      <th>SUB_STREET</th>\n      <th>STREET</th>\n      <th>SUB_LOCALITY</th>\n      <th>LOCALITY</th>\n      <th>TOWN</th>\n      <th>POSTCODE</th>\n      <th>group_id</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 147
        }
      ],
      "source": [
        "address_entity_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fuzzy match with the existing addresses\n",
        "def add_address_to_table(address_text,group_id,columns=columns):\n",
        "    global address_entity_table\n",
        "    address=AddressParser()\n",
        "    parsed_dict=address.get_parsed_address(address_text=address_text)\n",
        "    row={**columns, **parsed_dict}\n",
        "    row['group_id']=group_id\n",
        "    address_entity_table=address_entity_table.append(row, ignore_index=True)\n",
        "    print('address saved in the database')\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "address saved in the database\naddress saved in the database\naddress saved in the database\n"
          ]
        }
      ],
      "source": [
        "add_address_to_table(address_text='395 king street, aberdeen, ab24 5rp',group_id=0)\n",
        "add_address_to_table(address_text='152, longcroft lane, welwyen garden city, al8 6en',group_id=1)\n",
        "add_address_to_table(address_text='30 gresham street, london, england, ec2v 7qp',group_id=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  SUB_ORGANISATION ORGANISATION SUB_BUILDING_NAME SUB_BUILDING_NUMBER  \\\n",
              "0                                                                       \n",
              "1                                                                       \n",
              "2                                                                       \n",
              "\n",
              "  BUILDING_NAME BUILDING_NUMBER PO_BOX_NUMBER SUB_STREET          STREET  \\\n",
              "0                           395                              king street   \n",
              "1                           152                           longcroft lane   \n",
              "2                            30                           gresham street   \n",
              "\n",
              "  SUB_LOCALITY LOCALITY                 TOWN  POSTCODE  group_id  \n",
              "0                                   aberdeen  ab24 5rp         0  \n",
              "1                        welwyen garden city   al8 6en         1  \n",
              "2               england               london  ec2v 7qp         2  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SUB_ORGANISATION</th>\n      <th>ORGANISATION</th>\n      <th>SUB_BUILDING_NAME</th>\n      <th>SUB_BUILDING_NUMBER</th>\n      <th>BUILDING_NAME</th>\n      <th>BUILDING_NUMBER</th>\n      <th>PO_BOX_NUMBER</th>\n      <th>SUB_STREET</th>\n      <th>STREET</th>\n      <th>SUB_LOCALITY</th>\n      <th>LOCALITY</th>\n      <th>TOWN</th>\n      <th>POSTCODE</th>\n      <th>group_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>395</td>\n      <td></td>\n      <td></td>\n      <td>king street</td>\n      <td></td>\n      <td></td>\n      <td>aberdeen</td>\n      <td>ab24 5rp</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>152</td>\n      <td></td>\n      <td></td>\n      <td>longcroft lane</td>\n      <td></td>\n      <td></td>\n      <td>welwyen garden city</td>\n      <td>al8 6en</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>30</td>\n      <td></td>\n      <td></td>\n      <td>gresham street</td>\n      <td></td>\n      <td>england</td>\n      <td>london</td>\n      <td>ec2v 7qp</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 149
        }
      ],
      "source": [
        "# initial values\n",
        "address_entity_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fuzzy_match_address(parsed_address,columns=columns):\n",
        "    global address_entity_table\n",
        "    group_id=int()\n",
        "    score_collector=[]\n",
        "    for index, row in address_entity_table.iterrows():\n",
        "        row_score=0\n",
        "        for key in parsed_address.keys():\n",
        "            row_score=row_score+fuzz.partial_ratio(row[key],parsed_address[key])\n",
        "        score_collector.append(row_score/len(parsed_address)) # average the score, so max value will be 100\n",
        "        print('simialrity score: ',row_score/len(parsed_address))\n",
        "    \n",
        "    max_val=max(score_collector)\n",
        "    row={**columns, **parsed_address}\n",
        "\n",
        "    if max_val>=90: # grouped\n",
        "        group_id=address_entity_table['group_id'][score_collector.index(max_val)]\n",
        "        print('address grouped')\n",
        "        row['group_id']=group_id\n",
        "\n",
        "    else: # add as new entity assign new group_id\n",
        "        last=max(list(address_entity_table['group_id']))\n",
        "        print('new address discovered')\n",
        "        row['group_id']=last+1\n",
        "\n",
        "    address_entity_table=address_entity_table.append(row, ignore_index=True)\n",
        "    print('address saved in the database')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "def address_grouper(address_text):\n",
        "    global address_entity_table\n",
        "    address=AddressParser()\n",
        "    parsed_address=address.get_parsed_address(address_text=address_text)\n",
        "    fuzzy_match_address(parsed_address)\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY9XNU9h0sNL"
      },
      "source": [
        "# classifier_main_function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LToRo6WV0g8Y"
      },
      "source": [
        "def classifier_main_function(input_entity):\n",
        "    global model\n",
        "    global tokenizer\n",
        "    global serial_number_entity_table\n",
        "    global physical_good_entity_table\n",
        "    global location_entity_table\n",
        "    # prediction (this is the starting point of our full model)\n",
        "\n",
        "    sentence = [input_entity] # convert into list\n",
        "    sequences = tokenizer.texts_to_sequences(sentence)\n",
        "    padded = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='pre')\n",
        "    output_class=reversed_labels[np.argmax(model.predict(padded)[0], axis=-1)]\n",
        "    print('the input entity is classified as: ', output_class)\n",
        "\n",
        "\n",
        "    if output_class=='serial_number':\n",
        "        serial_grouper(input_entity)\n",
        "        print('-'*60)\n",
        "        print(serial_number_entity_table)\n",
        "\n",
        "    elif output_class=='physical_good':\n",
        "        goods_grouper(input_entity)\n",
        "        print('-'*60)\n",
        "        print(physical_good_entity_table)\n",
        "\n",
        "    elif output_class=='location':\n",
        "        location_grouper(input_entity)\n",
        "        print('-'*60)\n",
        "        print(location_entity_table)\n",
        "        \n",
        "    elif output_class=='company_name':\n",
        "        company_grouper(input_entity)\n",
        "        print('-'*60)\n",
        "        print(company_name_entity_table)\n",
        "\n",
        "    elif output_class=='company_address':\n",
        "        address_grouper(input_entity)\n",
        "        print('-'*60)\n",
        "        print(address_entity_table)\n",
        "\n",
        "    return\n",
        "    "
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08A2Z8lQ2VDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf1d8ac-2fae-4b8a-d034-8d62cb7366a0"
      },
      "source": [
        "# calling the master function\n",
        "input_entity=input('enter entity')\n",
        "classifier_main_function(input_entity)"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the input entity is classified as:  physical_good\n0.27178505063056946\n0.2989785969257355\n0.8444654941558838\n0.5819348692893982\nmax sim: 0.8444654941558838\nentity grouped:  {'entity': 'sofa', 'group_id': 2}\n------------------------------------------------------------\n           entity  group_id                                             vector\n0  plastic bottle         0  [-0.289565, -0.049595, -0.077429, -0.15209301,...\n1      steel bowl         1  [0.12778, 0.25831202, 0.45532, -0.372285, -0.1...\n2    leather sofa         2  [-0.06037, -0.40972, -0.208975, -0.005081499, ...\n3  hardwood table         3  [-0.091709, 0.09808999, -0.29916, -0.36364597,...\n4            sofa         2  [0.25532, -0.60838, -0.30667, 0.053605, -0.416...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}