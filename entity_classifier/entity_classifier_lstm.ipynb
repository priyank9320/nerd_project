{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "entity_classifier_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.4-final"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo7CtpewFt51"
      },
      "source": [
        "Description: \n",
        "First we train a Bi-LSTM character level classifier which is used to classify the entire input string into one of 5 categories. After classification, we try to group the entity with the already present entities according to simialrity criterias. If the entity is not similar enough to the already present entities it will be recorded as a new entity and assigned a new group id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3ktt5nmDCEq"
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install pywikibot\n",
        "!pip install jellyfish\n",
        "!pip install rapidfuzz\n",
        "!pip install autocorrect\n",
        "!pip install spacy==2.2.4\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /home/priyank/anaconda3/lib/python3.7/site-packages (2.4.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.15.7)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (41.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.28.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (2.25.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.1.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/priyank/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /home/priyank/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (0.23)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/priyank/anaconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.26.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2019.9.11)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/priyank/anaconda3/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: more-itertools in /home/priyank/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (7.2.0)\n",
            "Requirement already satisfied: pywikibot in /home/priyank/anaconda3/lib/python3.7/site-packages (6.0.1)\n",
            "Requirement already satisfied: requests>=2.20.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from pywikibot) (2.25.0)\n",
            "Requirement already satisfied: setuptools>=20.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from pywikibot) (41.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests>=2.20.1->pywikibot) (2.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests>=2.20.1->pywikibot) (1.26.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests>=2.20.1->pywikibot) (2019.9.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests>=2.20.1->pywikibot) (3.0.4)\n",
            "Requirement already satisfied: jellyfish in /home/priyank/anaconda3/lib/python3.7/site-packages (0.8.2)\n",
            "Requirement already satisfied: rapidfuzz in /home/priyank/anaconda3/lib/python3.7/site-packages (0.13.3)\n",
            "Requirement already satisfied: autocorrect in /home/priyank/anaconda3/lib/python3.7/site-packages (1.1.0)\n",
            "Requirement already satisfied: spacy==2.2.4 in /home/priyank/anaconda3/lib/python3.7/site-packages (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (41.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (2.25.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (4.55.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy==2.2.4) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /home/priyank/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (0.23)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/priyank/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (0.6.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (1.26.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2019.9.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (3.0.4)\n",
            "Requirement already satisfied: more-itertools in /home/priyank/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (7.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 15.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (41.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.55.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.25.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/priyank/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /home/priyank/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (0.23)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/priyank/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.26.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2019.9.11)\n",
            "Requirement already satisfied: more-itertools in /home/priyank/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (7.2.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /home/priyank/anaconda3/lib/python3.7/site-packages (2.4.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: absl-py~=0.10 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: wheel~=0.35 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.15.7)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (41.4.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.28.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (2.25.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /home/priyank/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.1.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/priyank/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /home/priyank/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (0.23)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/priyank/anaconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.26.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2019.9.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/priyank/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/priyank/anaconda3/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: more-itertools in /home/priyank/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (7.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2PakEB-nUln"
      },
      "source": [
        "import pywikibot\n",
        "from rapidfuzz import fuzz \n",
        "\n",
        "import en_core_web_lg\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "\n",
        "from scipy import spatial\n",
        "from autocorrect import Speller\n",
        "\n",
        "import re\n",
        "\n",
        "from googlesearch import search\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSY7fMTgDwTp"
      },
      "source": [
        "import string\n",
        "from random import randrange\n",
        "import random  # used to create random separator\n",
        "import string  # this is used to create the vocab of all the characters , stored in the variable 'vocab'\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "import csv\n",
        "import pickle\n",
        "\n",
        "# below packages are used to create the model\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Bidirectional, TimeDistributed, Input, Masking\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n070HCcGQBZe"
      },
      "source": [
        "# max length of the input\n",
        "maxlen=100"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9v4nP5R5mPa"
      },
      "source": [
        "# load the csv data file\n",
        "df=pd.read_csv('datafile_lstm.csv')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1c5g1hyRg6V"
      },
      "source": [
        "# shuffle data frame\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl0w9Aj12BYB"
      },
      "source": [
        "# split the dataframe into train and test\n",
        "df1=df.iloc[100:500,:]\n",
        "df2=df.iloc[:100,:]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv2_qn6e13xF"
      },
      "source": [
        "# Dictionary contains the Labels\n",
        "labels_dict = {  # we are keeping the value part as strings as it easy to generate labels , we could just give len(string)*label value to generate the label\n",
        "    'company_name': 0,\n",
        "    'company_address': 1,\n",
        "    'serial_number': 2,\n",
        "    'physical_good': 3,\n",
        "    'location': 4\n",
        "}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBGS7WJP18UR",
        "outputId": "2b0e5d7e-398a-4efa-ec2d-7970b3856065"
      },
      "source": [
        "# create reverse dictionary of label placeholders\n",
        "reversed_labels = {value: key for (key, value) in labels_dict.items()}\n",
        "reversed_labels"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'company_name',\n",
              " 1: 'company_address',\n",
              " 2: 'serial_number',\n",
              " 3: 'physical_good',\n",
              " 4: 'location'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2LvIHDsD50q"
      },
      "source": [
        "# define the vocab\n",
        "vocab = list(string.whitespace + string.digits +\n",
        "             string.ascii_lowercase + string.punctuation)  # total 74 characters"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxqdzJTkPzZH",
        "outputId": "8a304f34-7795-4bac-c8d1-4426f038fb24"
      },
      "source": [
        "vocab_size=len(vocab)\n",
        "vocab_size"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTje8-NnEJE1"
      },
      "source": [
        "# create the Tokenizer object, word_index contaning all the chars in the vocab\n",
        "# basically this will encode all the characters into numbers, so each character defined in our vocab will be represented by a number\n",
        "# vocab is just a list of tokens\n",
        "tokenizer = Tokenizer(num_words=len(vocab), filters='',\n",
        "                      char_level=True, oov_token='<oov>')\n",
        "tokenizer.fit_on_texts(vocab)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSHrIukISxdE"
      },
      "source": [
        "#train\n",
        "train_sentences=list(df1['text'])\n",
        "train_labels=list(df1['category'])\n",
        "train_sequences=tokenizer.texts_to_sequences(train_sentences)\n",
        "train_padded=pad_sequences(train_sequences, padding='post',maxlen=maxlen, truncating='pre') #maxlen decides the maximum length of the sequence\n",
        "train_labels=[labels_dict[item] for item in train_labels ]\n",
        "train_labels = np.asarray(train_labels, dtype=np.int32)\n",
        "train_label_one_hot = tf.keras.utils.to_categorical(train_labels, dtype='int32')\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRmyMQFhR6Hz"
      },
      "source": [
        "#test\n",
        "test_sentences=list(df2['text'])\n",
        "test_labels=list(df2['category'])\n",
        "test_sequences=tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded=pad_sequences(test_sequences, padding='post',maxlen=maxlen, truncating='pre') #maxlen decides the maximum length of the sequence\n",
        "test_labels=[labels_dict[item] for item in test_labels ]\n",
        "test_labels = np.asarray(test_labels, dtype=np.int32)\n",
        "test_label_one_hot = tf.keras.utils.to_categorical(test_labels, dtype='int32')\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcBFNuK1FKJm",
        "outputId": "6036396e-dd05-4927-f0df-fd33c4fd21d5"
      },
      "source": [
        "# Create model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size+1, output_dim=32,\n",
        "                    input_length=maxlen, mask_zero=True))\n",
        "# vocab_size+1 is to inlcude the padding\n",
        "# output_dim is the size of the embedding for the characters in the vocab\n",
        "model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
        "#model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
        "# 5 is the number of labels and which we need in the softamx classifier\n",
        "model.add(Dense(len(labels_dict), activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 100, 32)           2400      \n_________________________________________________________________\nbidirectional (Bidirectional (None, 64)                16640     \n_________________________________________________________________\ndense (Dense)                (None, 5)                 325       \n=================================================================\nTotal params: 19,365\nTrainable params: 19,365\nNon-trainable params: 0\n_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCOwOI99FSVq"
      },
      "source": [
        "# create checkpoint\n",
        "checkpoint_filepath = 'model_checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    save_best_only=False,\n",
        "    monitor='val_accuracy',\n",
        "    save_freq='epoch',\n",
        "    mode='max',\n",
        "    verbose=0,\n",
        "    options=None\n",
        ")\n",
        "adam = keras.optimizers.Adam(learning_rate=0.001015)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_padded, train_label_one_hot,batch_size=32, epochs=10,  # TRAIN THE MODEL\n",
        "                    validation_data=(test_padded, test_label_one_hot)#, callbacks=[model_checkpoint_callback]\n",
        "                    )\n",
        "model.save('model/intelligent-address-parser-model.h5')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "13/13 [==============================] - 15s 394ms/step - loss: 1.6018 - accuracy: 0.2834 - val_loss: 1.5797 - val_accuracy: 0.3600\n",
            "Epoch 2/10\n",
            "13/13 [==============================] - 1s 110ms/step - loss: 1.5592 - accuracy: 0.4672 - val_loss: 1.4895 - val_accuracy: 0.5400\n",
            "Epoch 3/10\n",
            "13/13 [==============================] - 1s 109ms/step - loss: 1.4051 - accuracy: 0.5779 - val_loss: 1.1979 - val_accuracy: 0.3700\n",
            "Epoch 4/10\n",
            "13/13 [==============================] - 1s 110ms/step - loss: 1.0720 - accuracy: 0.5156 - val_loss: 0.9908 - val_accuracy: 0.7100\n",
            "Epoch 5/10\n",
            "13/13 [==============================] - 1s 96ms/step - loss: 0.8396 - accuracy: 0.7350 - val_loss: 0.8083 - val_accuracy: 0.8100\n",
            "Epoch 6/10\n",
            "13/13 [==============================] - 1s 91ms/step - loss: 0.7092 - accuracy: 0.7737 - val_loss: 0.6859 - val_accuracy: 0.8100\n",
            "Epoch 7/10\n",
            "13/13 [==============================] - 1s 104ms/step - loss: 0.6035 - accuracy: 0.7805 - val_loss: 0.5946 - val_accuracy: 0.8600\n",
            "Epoch 8/10\n",
            "13/13 [==============================] - 1s 99ms/step - loss: 0.5046 - accuracy: 0.8292 - val_loss: 0.6112 - val_accuracy: 0.7600\n",
            "Epoch 9/10\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 0.4549 - accuracy: 0.8147 - val_loss: 0.4888 - val_accuracy: 0.8100\n",
            "Epoch 10/10\n",
            "13/13 [==============================] - 2s 190ms/step - loss: 0.3918 - accuracy: 0.8478 - val_loss: 0.4360 - val_accuracy: 0.8500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWVbup2dDG_A"
      },
      "source": [
        "# Entity Disambiguation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eElXBELguc3c"
      },
      "source": [
        "# load the english spacy model\n",
        "# this is used to get vectors for text\n",
        "nlp = en_core_web_lg.load()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehXXQGPs9-5W"
      },
      "source": [
        "# setting up the tables where data will be stored\n",
        "company_address_entity_table = pd.DataFrame({'entity':'','group_id':int()},index=[])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "vq25DgXeGtM5",
        "outputId": "52710541-90e6-41d7-cef6-26a1bb12ca7c"
      },
      "source": [
        "# sample serial numbers inital values\n",
        "serial_number_entity_table = pd.DataFrame({'entity':'','group_id':int()},index=[])\n",
        "\n",
        "serial_number_entity_table=pd.DataFrame({'entity':['abc12345','bhkdj9849204','zlljdoi9720483','bbnvnv909090'],\n",
        "                                   'group_id':[0,1,2,3]})\n",
        "serial_number_entity_table"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           entity  group_id\n",
              "0        abc12345         0\n",
              "1    bhkdj9849204         1\n",
              "2  zlljdoi9720483         2\n",
              "3    bbnvnv909090         3"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>entity</th>\n      <th>group_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>abc12345</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>bhkdj9849204</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>zlljdoi9720483</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>bbnvnv909090</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "qD1-aJ_Bol-3",
        "outputId": "ba9c6353-1c5a-402f-8f3e-58631e5c3b03"
      },
      "source": [
        "# sample physical goods initial values\n",
        "physical_good_entity_table = pd.DataFrame({'entity':'','group_id':int()},index=[])\n",
        "\n",
        "physical_good_entity_table=physical_good_entity_table.append({'entity':'plastic bottle','group_id':0,'vector':nlp('plastic bottle').vector},ignore_index=True)\n",
        "physical_good_entity_table=physical_good_entity_table.append({'entity':'steel bowl','group_id':1,'vector':nlp('steel bowl').vector},ignore_index=True)\n",
        "physical_good_entity_table=physical_good_entity_table.append({'entity':'leather sofa','group_id':2,'vector':nlp('leather sofa').vector},ignore_index=True)\n",
        "physical_good_entity_table=physical_good_entity_table.append({'entity':'hardwood table','group_id':3,'vector':nlp('hardwood table').vector},ignore_index=True)\n",
        "\n",
        "physical_good_entity_table"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           entity  group_id                                             vector\n",
              "0  plastic bottle         0  [-0.289565, -0.049595, -0.077429, -0.15209301,...\n",
              "1      steel bowl         1  [0.12778, 0.25831202, 0.45532, -0.372285, -0.1...\n",
              "2    leather sofa         2  [-0.06037, -0.40972, -0.208975, -0.005081499, ...\n",
              "3  hardwood table         3  [-0.091709, 0.09808999, -0.29916, -0.36364597,..."
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>entity</th>\n      <th>group_id</th>\n      <th>vector</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>plastic bottle</td>\n      <td>0</td>\n      <td>[-0.289565, -0.049595, -0.077429, -0.15209301,...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>steel bowl</td>\n      <td>1</td>\n      <td>[0.12778, 0.25831202, 0.45532, -0.372285, -0.1...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>leather sofa</td>\n      <td>2</td>\n      <td>[-0.06037, -0.40972, -0.208975, -0.005081499, ...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>hardwood table</td>\n      <td>3</td>\n      <td>[-0.091709, 0.09808999, -0.29916, -0.36364597,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "LIe3en6btXiy",
        "outputId": "9beb3f12-1e97-4838-dda6-81baff5b7b23"
      },
      "source": [
        "# sample location initial value\n",
        "location_entity_table = pd.DataFrame({'entity':'','group_id':int()},index=[])\n",
        "\n",
        "location_entity_table=location_entity_table.append({'entity':'London','group_id':0,'vector':nlp('London, UK').vector},ignore_index=True)\n",
        "location_entity_table=location_entity_table.append({'entity':'Rome, Italy','group_id':1,'vector':nlp('Rome, Italy').vector},ignore_index=True)\n",
        "location_entity_table=location_entity_table.append({'entity':'Tokyo','group_id':2,'vector':nlp('Tokyo').vector},ignore_index=True)\n",
        "location_entity_table=location_entity_table.append({'entity':'Japan','group_id':3,'vector':nlp('Japan').vector},ignore_index=True)\n",
        "\n",
        "location_entity_table"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        entity  group_id                                             vector\n",
              "0       London         0  [-0.040734004, 0.24897666, 0.082936674, -0.140...\n",
              "1  Rome, Italy         1  [0.041836005, 0.30547366, -0.11563143, -0.2124...\n",
              "2        Tokyo         2  [0.28876, -0.55541, 0.083178, -0.19359, 0.3757...\n",
              "3        Japan         3  [-0.44528, -0.17553, 0.075346, 0.0048481, 0.23..."
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>entity</th>\n      <th>group_id</th>\n      <th>vector</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>London</td>\n      <td>0</td>\n      <td>[-0.040734004, 0.24897666, 0.082936674, -0.140...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>Rome, Italy</td>\n      <td>1</td>\n      <td>[0.041836005, 0.30547366, -0.11563143, -0.2124...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>Tokyo</td>\n      <td>2</td>\n      <td>[0.28876, -0.55541, 0.083178, -0.19359, 0.3757...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>Japan</td>\n      <td>3</td>\n      <td>[-0.44528, -0.17553, 0.075346, 0.0048481, 0.23...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YnKifKQjCeY"
      },
      "source": [
        "# serial_grouper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XVmZlpg-sQH"
      },
      "source": [
        "#decide to group or to add as new entity\n",
        "#from rapidfuzz import fuzz\n",
        "\n",
        "def serial_grouper(input_string):\n",
        "    global serial_number_entity_table\n",
        "    #input_string=sentence\n",
        "    grouped_flag=0\n",
        "    for index, row in serial_number_entity_table.iterrows():\n",
        "\n",
        "        if re.findall(\"\\A\"+input_string[0:3], row['entity']): # check if the starting 3 characters are same, if yes then group them\n",
        "            # save the entity in table and assign the same group id\n",
        "            serial_number_entity_table=serial_number_entity_table.append({'entity':input_string, 'group_id':row['group_id']},ignore_index=True)\n",
        "            print('entity grouped: ',{'entity':input_string, 'group_id':row['group_id']})\n",
        "            grouped_flag=1\n",
        "            break\n",
        "        \n",
        "\n",
        "    if grouped_flag==0:\n",
        "        # save it as a new entity\n",
        "        last=max(list(serial_number_entity_table['group_id']))\n",
        "        serial_number_entity_table=serial_number_entity_table.append({'entity':input_string,'group_id':last+1},ignore_index=True)\n",
        "        print('entity saved as new entry in the table: ', {'entity':input_string,'group_id':last+1})  \n",
        "        \n",
        "    return"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tEd9z1Yi6xj"
      },
      "source": [
        "#goods_grouper\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPNUp8hHxx1i"
      },
      "source": [
        "# importing spelling corrector, we can easily use spelling correction as physical goods are very common and new names dont come in this\n",
        "spell = Speller(lang='en')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrL8KHWGl7Qc"
      },
      "source": [
        "def goods_grouper(input_entity):\n",
        "    global physical_good_entity_table\n",
        "    #input_entity='chair'\n",
        "    input_entity=spell(input_entity) # spelling correction # physical goods have very general name and so we can easily apply spelling correction\n",
        "\n",
        "    input_vector=nlp(input_entity).vector\n",
        "    list_of_similarities=[]\n",
        "    for index, row in physical_good_entity_table.iterrows():\n",
        "        result = 1 - spatial.distance.cosine(row['vector'], input_vector )\n",
        "        print(result)\n",
        "        list_of_similarities.append(result)\n",
        "\n",
        "    max_sim=max(list_of_similarities)\n",
        "    print('max sim:',max_sim)\n",
        "    if max_sim>0.72:\n",
        "        group_id=list_of_similarities.index(max_sim)\n",
        "        physical_good_entity_table=physical_good_entity_table.append({'entity':input_entity,'group_id':group_id,'vector':input_vector},ignore_index=True)\n",
        "        print('entity grouped: ',{'entity':input_entity,'group_id':group_id})\n",
        "\n",
        "    else:\n",
        "        last=max(list(physical_good_entity_table['group_id']))\n",
        "        physical_good_entity_table=physical_good_entity_table.append({'entity':input_entity,'group_id':last+1,'vector':input_vector},ignore_index=True)\n",
        "        print('entity added: ',{'entity':input_entity,'group_id':last+1})\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHSjO6aoB-UP"
      },
      "source": [
        "# location_grouper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO2GazHZPddU"
      },
      "source": [
        "vectors are helpful in reducing the search space, or you can say it helps to get candidates then we do string comparisions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdQBGVm2Ck7w"
      },
      "source": [
        "#spell = Speller(lang='en') # spelling correction was not working properly over here\n",
        "\n",
        "def location_grouper(input_entity):\n",
        "    global location_entity_table\n",
        "    #input_entity='London, UK'\n",
        "    #input_entity=spell(input_entity) # spelling correction # physical goods have very general name and so we can easily apply spelling correction\n",
        "\n",
        "    input_vector=nlp(input_entity).vector\n",
        "    list_of_similarities=[]\n",
        "    candidates=[]\n",
        "    for index, row in location_entity_table.iterrows():\n",
        "        result = 1 - spatial.distance.cosine(row['vector'], input_vector )\n",
        "        print(result)\n",
        "        if result>0.70:\n",
        "            candidates.append(index)\n",
        "        #list_of_similarities.append(result)\n",
        "\n",
        "    print('candidates:',candidates)\n",
        "\n",
        "    if candidates:\n",
        "        # prep input_entity for comparision by splitting it at spaces and\n",
        "        input_entity_modified = input_entity.split(',')\n",
        "        print('input_entity_modified',input_entity_modified)\n",
        "        in_vec_list=[] # list of individual vectors for the entity parts\n",
        "        for item in input_entity_modified:\n",
        "            in_vec_list.append(nlp(item.strip()).vector)\n",
        "            \n",
        "\n",
        "        # for each candidate , replace comma with space and split at space, and then compare the vectors\n",
        "\n",
        "        candidate_score_list=[]\n",
        "        for item in candidates :\n",
        "            can_string=location_entity_table['entity'][item] # replace comma with space\n",
        "            can_string=can_string.split(',')\n",
        "\n",
        "            print('can_string: ',can_string)\n",
        "            can_vec_list=[] # list of individual vectors for the entity parts\n",
        "            for i in can_string:\n",
        "                can_vec_list.append(nlp(i.strip()).vector)\n",
        "\n",
        "            # now we compare the elements from the 2 vector lists\n",
        "            print('in_vec_list length',len(in_vec_list))\n",
        "            print('can_vec_list length',len(can_vec_list))\n",
        "\n",
        "            if len(in_vec_list)==1 and len(can_vec_list)==1:\n",
        "                candidate_score_list.append(1 - spatial.distance.cosine(can_vec_list[0], in_vec_list[0]))\n",
        "\n",
        "            elif len(in_vec_list)==1 and len(can_vec_list)==2:# first part is important\n",
        "                candidate_score_list.append(1 - spatial.distance.cosine(can_vec_list[0], in_vec_list[0]))\n",
        "\n",
        "            elif len(in_vec_list)==2 and len(can_vec_list)==1: # first part is important\n",
        "                print('executing : in2 and can1')\n",
        "                candidate_score_list.append(1 - spatial.distance.cosine(can_vec_list[0], in_vec_list[0]))\n",
        "\n",
        "            elif len(in_vec_list)==2 and len(can_vec_list)==2: # least score wil determine \n",
        "                candidate_score_list.append(min([1 - spatial.distance.cosine(can_vec_list[0], in_vec_list[0]), 1 - spatial.distance.cosine(can_vec_list[1], in_vec_list[1])]))\n",
        "\n",
        "            else:\n",
        "                print('no criteria executed')\n",
        "\n",
        "\n",
        "        print('candidate_score_list: ',candidate_score_list)\n",
        "        max_sim=max(candidate_score_list)\n",
        "\n",
        "        print('max sim:',max_sim)\n",
        "        if max_sim>0.85:\n",
        "            group_id=location_entity_table['group_id'][candidates[candidate_score_list.index(max_sim)]]\n",
        "            location_entity_table=location_entity_table.append({'entity':input_entity,'group_id':group_id,'vector':input_vector},ignore_index=True)\n",
        "            print('entity grouped: ',{'entity':input_entity,'group_id':group_id})\n",
        "\n",
        "        else:\n",
        "            last=max(list(location_entity_table['group_id']))\n",
        "            location_entity_table=location_entity_table.append({'entity':input_entity,'group_id':last+1,'vector':input_vector},ignore_index=True)\n",
        "            print('entity added: ',{'entity':input_entity,'group_id':last+1})\n",
        "\n",
        "    else:\n",
        "        last=max(list(location_entity_table['group_id']))\n",
        "        location_entity_table=location_entity_table.append({'entity':input_entity,'group_id':last+1,'vector':input_vector},ignore_index=True)\n",
        "        print('entity added: ',{'entity':input_entity,'group_id':last+1})"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCwJLWbeAE7S"
      },
      "source": [
        "# company_grouper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_urls(tag, n, language):\n",
        "    urls = [url for url in search(tag, stop=n, lang=language)]\n",
        "    return urls\n",
        "\n",
        "#get_urls('glassmkaing companies uk list',10,'en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_aliases(input_company):\n",
        "    #input_company='m&s'\n",
        "    alias_list=[]\n",
        "    try:\n",
        "        link=get_urls(input_company+' company wikipedia page',1,'en')[0]\n",
        "        print('link from google: ',link)\n",
        "        if 'wikipedia' in link:\n",
        "            link=re.sub('.+wiki\\/','',link)\n",
        "        print('cleaned name from link: ',link)\n",
        "        input_company=link\n",
        "        site = pywikibot.Site(\"en\", \"wikipedia\")\n",
        "        page = pywikibot.Page(site,input_company)\n",
        "        item = pywikibot.ItemPage.fromPage(page)\n",
        "        item_dict = item.get()\n",
        "        alias_list=item_dict['aliases']['en']\n",
        "    except Exception as e:\n",
        "        print('error in google serp api: ',e)\n",
        "        alias_list=[]\n",
        "\n",
        "\n",
        "    return alias_list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfjBheVsAIUz"
      },
      "source": [
        "def add_company_aliases_to_table(input_company,group_id):\n",
        "    input_company=input_company.lower()\n",
        "    global company_name_entity_table\n",
        "    alias_list=[]\n",
        "    flag=True\n",
        "\n",
        "    try:\n",
        "\n",
        "        alias_list=find_aliases(input_company)\n",
        "\n",
        "        #check similarity of the input entit yand the aliases found for that entity to make sure we have the right page\n",
        "        for i in alias_list:\n",
        "\n",
        "            #print(input_company, i)\n",
        "\n",
        "            if fuzz.partial_ratio(input_company, i.lower())>=90:\n",
        "                flag=True\n",
        "                print('wiki page found is relevant')\n",
        "                break\n",
        "            else:\n",
        "                flag=False\n",
        "\n",
        "        if flag==False:\n",
        "            raise Exception('wiki page aliases didnt match so adding the enitity as new entry in table')\n",
        "\n",
        "        if input_company not in alias_list:\n",
        "            alias_list.append(input_company)\n",
        "            \n",
        "        for i in alias_list:\n",
        "            company_name_entity_table=company_name_entity_table.append({'entity':i.lower(),'group_id':group_id},ignore_index=True)\n",
        "\n",
        "    except Exception as e:\n",
        "            print('error occurred: ',e)\n",
        "            print('adding the entity as new entry')\n",
        "            company_name_entity_table=company_name_entity_table.append({'entity':input_company,'group_id':group_id},ignore_index=True)\n",
        "\n",
        "    return"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "company_name_entity_table = pd.DataFrame({'entity':'','group_id':int()},index=[])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbS4M4aSAenc",
        "tags": []
      },
      "source": [
        "# add values in the sample database\n",
        "add_company_aliases_to_table(input_company='walmart',group_id=0)\n",
        "add_company_aliases_to_table(input_company='tesco',group_id=1)\n",
        "add_company_aliases_to_table(input_company='google',group_id=2)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "link from google:  https://en.wikipedia.org/wiki/Walmart\n",
            "cleaned name from link:  Walmart\n",
            "walmart Wal-Mart\n",
            "walmart Wal Mart\n",
            "walmart Wal-Mart Stores, Inc.\n",
            "walmart Walmart, Inc.\n",
            "wiki page found is relevant\n",
            "link from google:  https://en.wikipedia.org/wiki/Tesco\n",
            "cleaned name from link:  Tesco\n",
            "tesco Tesco PLC\n",
            "wiki page found is relevant\n",
            "link from google:  https://en.wikipedia.org/wiki/Google\n",
            "cleaned name from link:  Google\n",
            "google Google Inc.\n",
            "wiki page found is relevant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQJhM2gHAiUm"
      },
      "source": [
        "company_name_entity_table"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   entity  group_id\n",
              "0                wal-mart         0\n",
              "1                wal mart         0\n",
              "2   wal-mart stores, inc.         0\n",
              "3           walmart, inc.         0\n",
              "4            walmart inc.         0\n",
              "5                wallmart         0\n",
              "6                wal-mart         0\n",
              "7               wall mart         0\n",
              "8               wall-mart         0\n",
              "9                 walmart         0\n",
              "10              tesco plc         1\n",
              "11                  tesco         1\n",
              "12            google inc.         2\n",
              "13             google llc         2\n",
              "14                 google         2"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>entity</th>\n      <th>group_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>wal-mart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>wal mart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>wal-mart stores, inc.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>walmart, inc.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>walmart inc.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>wallmart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>wal-mart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>wall mart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>wall-mart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>walmart</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>tesco plc</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>tesco</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>google inc.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>google llc</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>google</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ38nWBNAjMl"
      },
      "source": [
        "def company_grouper(input_company):\n",
        "    input_company=input_company.lower()\n",
        "    global company_name_entity_table\n",
        "    flag=False\n",
        "    for index, row in company_name_entity_table.iterrows():\n",
        "        if fuzz.partial_ratio(input_company, row['entity'])>=90:\n",
        "            #print(input_company, row['entity'])\n",
        "            flag=True\n",
        "            group_id=row['group_id']\n",
        "            company_name_entity_table=company_name_entity_table.append({'entity':input_company,'group_id':group_id},ignore_index=True)\n",
        "            print('entity grouped: ',{'entity':input_company,'group_id':group_id})\n",
        "            break\n",
        "        else:\n",
        "            flag=False\n",
        "            pass\n",
        "\n",
        "    if flag==False:\n",
        "        print('entity not present in the existing database, trying to find aliases on wikidata before adding it')\n",
        "        last=max(list(company_name_entity_table['group_id']))\n",
        "        add_company_aliases_to_table(input_company=input_company,group_id=last+1)\n",
        "\n",
        "    return \n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTuv_K59Am8f"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY9XNU9h0sNL"
      },
      "source": [
        "# classifier_main_function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LToRo6WV0g8Y"
      },
      "source": [
        "def classifier_main_function(input_entity):\n",
        "    global model\n",
        "    global tokenizer\n",
        "    global serial_number_entity_table\n",
        "    global physical_good_entity_table\n",
        "    global location_entity_table\n",
        "    # prediction (this is the starting point of our full model)\n",
        "\n",
        "    sentence = [input_entity] # convert into list\n",
        "    sequences = tokenizer.texts_to_sequences(sentence)\n",
        "    padded = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='pre')\n",
        "    output_class=reversed_labels[np.argmax(model.predict(padded)[0], axis=-1)]\n",
        "    print('the input entity is classified as: ', output_class)\n",
        "\n",
        "\n",
        "    if output_class=='serial_number':\n",
        "        serial_grouper(input_entity)\n",
        "        print('-'*60)\n",
        "        print(serial_number_entity_table)\n",
        "\n",
        "    elif output_class=='physical_good':\n",
        "        goods_grouper(input_entity)\n",
        "        print('-'*60)\n",
        "        print(physical_good_entity_table)\n",
        "\n",
        "    elif output_class=='location':\n",
        "        location_grouper(input_entity)\n",
        "        print('-'*60)\n",
        "        print(location_entity_table)\n",
        "        \n",
        "    elif output_class=='company_name':\n",
        "        company_grouper(input_entity)\n",
        "        print('-'*60)\n",
        "        print(company_name_entity_table)\n",
        "\n",
        "    return\n",
        "    "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08A2Z8lQ2VDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf1d8ac-2fae-4b8a-d034-8d62cb7366a0"
      },
      "source": [
        "# calling the master function\n",
        "input_entity=input('enter entity')\n",
        "classifier_main_function(input_entity)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the input entity is classified as:  company_name\n",
            "entity not present in the existing database, trying to find aliases on wikidata before adding it\n",
            "link from google:  https://en.wikipedia.org/wiki/IBM\n",
            "cleaned name from link:  IBM\n",
            "wiki page found is relevant\n",
            "------------------------------------------------------------\n",
            "                                         entity  group_id\n",
            "0                                      wal-mart         0\n",
            "1                                      wal mart         0\n",
            "2                         wal-mart stores, inc.         0\n",
            "3                                 walmart, inc.         0\n",
            "4                                  walmart inc.         0\n",
            "5                                      wallmart         0\n",
            "6                                      wal-mart         0\n",
            "7                                     wall mart         0\n",
            "8                                     wall-mart         0\n",
            "9                                       walmart         0\n",
            "10                                    tesco plc         1\n",
            "11                                        tesco         1\n",
            "12                                  google inc.         2\n",
            "13                                   google llc         2\n",
            "14                                       google         2\n",
            "15                                      ibm plc         3\n",
            "16  international business machines corporation         4\n",
            "17                                     big blue         4\n",
            "18                                       i.b.m.         4\n",
            "19              international business machines         4\n",
            "20                              ibm corporation         4\n",
            "21                                    ibm corp.         4\n",
            "22                                          18m         4\n",
            "23          international business machines plc         4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}